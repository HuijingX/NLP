{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "蚂蚁金融语义相似度_ESIM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HI6-CMZsCBfe"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def read_data(path):\n",
        "  sentence_a = []\n",
        "  sentence_b = []\n",
        "  labels = []\n",
        "  with open(path, 'r', encoding='utf8') as f:\n",
        "    for line in tqdm(f.readlines(), desc='Reading data'):\n",
        "      line = json.loads(line)\n",
        "      sentence_a.append(line['sentence1'])\n",
        "      sentence_b.append(line['sentence2'])\n",
        "      labels.append(int(line['label']))\n",
        "\n",
        "  df = pd.DataFrame(zip(sentence_a,sentence_b,labels), columns=['text_a', 'text_b', 'labels'])\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwkKFBYAHTRP",
        "outputId": "b493a304-5719-41be-90df-d2c9a39affbb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Reading data: 100%|██████████| 34334/34334 [00:00<00:00, 213443.66it/s]\n"
          ]
        }
      ],
      "source": [
        "train_df = read_data('/content/drive/MyDrive/train.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "cl5Ex0TcHmhw",
        "outputId": "61fe2baa-27cb-4df6-85ff-e6b2e670a16b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-59b62717-57c2-4cd4-9957-35071f25ddf2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_a</th>\n",
              "      <th>text_b</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>蚂蚁借呗等额还款可以换成先息后本吗</td>\n",
              "      <td>借呗有先息到期还本吗</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>蚂蚁花呗说我违约一次</td>\n",
              "      <td>蚂蚁花呗违约行为是什么</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>帮我看一下本月花呗账单有没有结清</td>\n",
              "      <td>下月花呗账单</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>蚂蚁借呗多长时间综合评估一次</td>\n",
              "      <td>借呗得评估多久</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>我的花呗账单是***，还款怎么是***</td>\n",
              "      <td>我的花呗，月结出来说让我还***元，我自己算了一下详细名单我应该还***元</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-59b62717-57c2-4cd4-9957-35071f25ddf2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-59b62717-57c2-4cd4-9957-35071f25ddf2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-59b62717-57c2-4cd4-9957-35071f25ddf2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                text_a                                 text_b  labels\n",
              "0    蚂蚁借呗等额还款可以换成先息后本吗                             借呗有先息到期还本吗       0\n",
              "1           蚂蚁花呗说我违约一次                            蚂蚁花呗违约行为是什么       0\n",
              "2     帮我看一下本月花呗账单有没有结清                                 下月花呗账单       0\n",
              "3       蚂蚁借呗多长时间综合评估一次                                借呗得评估多久       0\n",
              "4  我的花呗账单是***，还款怎么是***  我的花呗，月结出来说让我还***元，我自己算了一下详细名单我应该还***元       1"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIWN-401IigA",
        "outputId": "0b08f7cf-f08c-438b-c32e-4cdd7a2dcdfc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Reading data: 100%|██████████| 4316/4316 [00:00<00:00, 209160.31it/s]\n"
          ]
        }
      ],
      "source": [
        "dev_df = read_data('/content/drive/MyDrive/dev.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "XeI7PWl4IpUB",
        "outputId": "2cb12e16-c95e-41a7-e9a4-95e7fa1ae9c5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4f72e7ef-be2c-481c-8e71-f91afb8a4d85\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_a</th>\n",
              "      <th>text_b</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>双十一花呗提额在哪</td>\n",
              "      <td>里可以提花呗额度</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>花呗支持高铁票支付吗</td>\n",
              "      <td>为什么友付宝不支持花呗付款</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>我的蚂蚁花呗支付金额怎么会有限制</td>\n",
              "      <td>我到支付宝实体店消费用花呗支付受金额限制</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>为什么有花呗额度不能分期付款</td>\n",
              "      <td>花呗分期额度不足</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>赠品不能设置用花呗付款</td>\n",
              "      <td>怎么不能花呗分期付款</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4f72e7ef-be2c-481c-8e71-f91afb8a4d85')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4f72e7ef-be2c-481c-8e71-f91afb8a4d85 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4f72e7ef-be2c-481c-8e71-f91afb8a4d85');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "             text_a                text_b  labels\n",
              "0         双十一花呗提额在哪              里可以提花呗额度       0\n",
              "1        花呗支持高铁票支付吗         为什么友付宝不支持花呗付款       0\n",
              "2  我的蚂蚁花呗支付金额怎么会有限制  我到支付宝实体店消费用花呗支付受金额限制       1\n",
              "3    为什么有花呗额度不能分期付款              花呗分期额度不足       0\n",
              "4       赠品不能设置用花呗付款            怎么不能花呗分期付款       0"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dev_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "aA3Yp-RHIsWO",
        "outputId": "fdc8d704-8d30-4635-be9d-ca2be7af1a1f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "(train_df.text_a.str.len() + train_df.text_b.str.len()).hist(bins=20);\n",
        "(dev_df.text_a.str.len() + dev_df.text_b.str.len()).hist(bins=20);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZN_KjbcbJBGX",
        "outputId": "8e13652c-259d-4416-9632-95453a2dceb7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "64.0"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(train_df.text_a.str.len() + train_df.text_b.str.len()).quantile(0.99)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6M1Z74UJL_F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "config = {\n",
        "    'train_file_path': '/content/drive/MyDrive/train.json',\n",
        "    'dev_file_path': '/content/drive/MyDrive/dev.json',\n",
        "    'test_file_path': '/content/drive/MyDrive/test.json',\n",
        "    'emdedding_file_path': '/content/drive/MyDrive/sgns.weibo.word.bz2',\n",
        "    'train_val_ratio': 0.1,\n",
        "    'vocab_size': 30000,\n",
        "    'batch_size': 64,\n",
        "    'max_seq_len': 64,\n",
        "    'num_epoches': 1,\n",
        "    'learning_rate': 1e-3,\n",
        "    'logging_step': 300,\n",
        "    'seed': 2021\n",
        "}\n",
        "\n",
        "config['device'] = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def seed_everything(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(config['seed'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWgUHMT3KQCe"
      },
      "outputs": [],
      "source": [
        "import jieba\n",
        "from collections import Counter\n",
        "def preprocess(config):\n",
        "  def convert2df(file_path, dataset='train'):\n",
        "    sentence_a = []\n",
        "    sentence_b = []\n",
        "    labels = []\n",
        "    with open(file_path, 'r', encoding='utf8') as f:\n",
        "      for line in tqdm(f.readlines(), desc=f'Reading {dataset} data'):\n",
        "        line = json.loads(line)\n",
        "        sentence_a.append(line['sentence1'])\n",
        "        sentence_b.append(line['sentence2'])\n",
        "        if dataset != 'test':\n",
        "          labels.append(int(line['label']))\n",
        "        else:\n",
        "          labels.append(0)\n",
        "\n",
        "        tokens = list(jieba.cut(sentence_a[-1])) + list(jieba.cut(sentence_b[-1]))\n",
        "        token_counter.update(tokens)\n",
        "\n",
        "    df = pd.DataFrame(zip(sentence_a, sentence_b, labels), columns=['text_a', 'text_b', 'lables'])\n",
        "    return df\n",
        "  \n",
        "  token_counter = Counter()\n",
        "\n",
        "  train_df = convert2df(config['train_file_path'], 'train')\n",
        "  dev_df = convert2df(config['dev_file_path'], 'dev')\n",
        "  test_df = convert2df(config['test_file_path'], 'test')\n",
        "\n",
        "  train_df = train_df.append(dev_df)\n",
        "\n",
        "  vocab = set(token for token,_ in token_counter.most_common(config['vocab_size']))\n",
        "  return train_df, test_df, vocab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--C4OVsLNSq-",
        "outputId": "3bcac32b-38c2-40c5-dd4a-1a9c76f9cb20"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Reading train data: 100%|██████████| 34334/34334 [00:08<00:00, 4245.98it/s]\n",
            "Reading dev data: 100%|██████████| 4316/4316 [00:01<00:00, 4147.28it/s]\n",
            "Reading test data: 100%|██████████| 3861/3861 [00:00<00:00, 4026.75it/s]\n"
          ]
        }
      ],
      "source": [
        "train_df, test_df, vocab = preprocess(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s6WnDtiPyYd"
      },
      "outputs": [],
      "source": [
        "import bz2\n",
        "def get_embedding(vocab):\n",
        "  token2embedding = {}\n",
        "  \n",
        "  with bz2.open('/content/drive/MyDrive/sgns.weibo.word.bz2') as f:\n",
        "    token_vector = f.readlines()\n",
        "\n",
        "    meta_info = token_vector[0].split()\n",
        "    print(f'{meta_info[0]} tokens in vectors file in total, vector size is {meta_info[1]}')\n",
        "\n",
        "    for line in tqdm(token_vector[1:]):\n",
        "      line = line.split()\n",
        "      token = line[0].decode('utf8')\n",
        "      vector = line[1:]\n",
        "\n",
        "      if token in vocab:\n",
        "        token2embedding[token] = [float(num) for num in vector] # 转换数据类型     \n",
        "\n",
        "    token2id = {token: idx for idx, token in enumerate(token2embedding.keys(),4)}\n",
        "    id2embedding = {token2id[token]: embedding for token, embedding in token2embedding.items()}\n",
        "\n",
        "    PAD, UNK, BOS, EOS = '<pad>', '<unk>', '<bos>', '<eos>'\n",
        "    token2id[PAD] = 0\n",
        "    token2id[UNK] = 1\n",
        "    token2id[BOS] = 2\n",
        "    token2id[EOS] = 3\n",
        "\n",
        "    id2embedding[0] = [.0] * int(meta_info[1])\n",
        "    id2embedding[1] = [.0] * int(meta_info[1])\n",
        "    id2embedding[2] = np.random.random(int(meta_info[1])).tolist()\n",
        "    id2embedding[3] = np.random.random(int(meta_info[1])).tolist()\n",
        "\n",
        "    emb_mat = [id2embedding[idx] for idx in range(len(id2embedding))] \n",
        "\n",
        "    return torch.tensor(emb_mat, dtype=torch.float), token2id, len(vocab)+4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTtVqYrpQFeY",
        "outputId": "478a5a79-952a-4d81-d1de-9fc7dcc49312"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'195202' tokens in vectors file in total, vector size is b'300'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 195202/195202 [00:03<00:00, 50461.23it/s]\n"
          ]
        }
      ],
      "source": [
        "emb_mat, token2id, config['vocab_size'] = get_embedding(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R-Ck3nNQJ_R"
      },
      "outputs": [],
      "source": [
        "def tokenizer(sent, token2id):\n",
        "  ids = [token2id.get(token,1) for token in jieba.cut(sent)]\n",
        "  return ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjLYsed4QZgU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "def read_data(data_df, train_val_ratio, token2id, mode='train'):\n",
        "\n",
        "  if mode == 'train':\n",
        "    X_train, y_train = defaultdict(list), []\n",
        "    X_val, y_val = defaultdict(list), []\n",
        "\n",
        "    num_val = int(config['train_val_ratio'] * len(data_df))\n",
        "  \n",
        "  else:\n",
        "    X_test, y_test = defaultdict(list), []\n",
        "\n",
        "  for i, row in tqdm(data_df.iterrows(), desc=f'Preprocesing {mode} data', total=len(data_df)):\n",
        "    # ---------------------------------------------#\n",
        "    text_left = row[0]\n",
        "    text_right = row[1]\n",
        "    label = row[2]\n",
        "\n",
        "    inputs_a = tokenizer(text_left, token2id)\n",
        "    inputs_b = tokenizer(text_right, token2id)\n",
        "\n",
        "    if mode == 'train':\n",
        "      if i < num_val:\n",
        "        X_val['text_left'].append(inputs_a)\n",
        "        X_val['text_right'].append(inputs_b)\n",
        "        y_val.append(label)\n",
        "      else:\n",
        "        X_train['text_left'].append(inputs_a)\n",
        "        X_train['text_right'].append(inputs_b)\n",
        "        y_train.append(label)\n",
        "\n",
        "    else:\n",
        "      X_test['text_left'].append(inputs_a)\n",
        "      X_test['text_right'].append(inputs_b)\n",
        "      y_test.append(label)\n",
        "    # ---------------------------------------------#\n",
        "\n",
        "  if mode == 'train':\n",
        "    label2id = {label: i for i, label in enumerate(np.unique(y_train))}\n",
        "    id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "    y_train = torch.tensor([label2id[label] for label in y_train], dtype=torch.long)\n",
        "    y_val = torch.tensor([label2id[label] for label in y_val], dtype=torch.long)\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, label2id, id2label\n",
        "  \n",
        "  else:\n",
        "    y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "    return X_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snR9vHPKQZim",
        "outputId": "f6e12c82-306b-4abf-acad-5d0fefc0ae7d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preprocesing train data: 100%|██████████| 38650/38650 [00:12<00:00, 3007.72it/s]\n",
            "Preprocesing test data: 100%|██████████| 3861/3861 [00:01<00:00, 3114.67it/s]\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train, X_val, y_val, label2id, id2label = read_data(train_df, config['train_val_ratio'], token2id, mode='train')\n",
        "X_test, y_test = read_data(test_df, config['train_val_ratio'], token2id, mode='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4iB_w9cQZk2"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "class AFQMCDataset(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    super(AFQMCDataset, self).__init__()\n",
        "    self.x = X\n",
        "    self.y = y\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # ---------------------------------------------#\n",
        "    data = (self.x['text_left'][idx], self.x['text_right'][idx], self.y[idx])\n",
        "    return data\n",
        "    # ---------------------------------------------#\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.y.size(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgA3UC9OUzp6"
      },
      "outputs": [],
      "source": [
        "class Collator():\n",
        "  def __init__(self, max_seq_len):\n",
        "    self.max_seq_len = max_seq_len\n",
        "\n",
        "  def get_max_seq_len(self, ids_list):\n",
        "    cur_max_seq_len = max(len(input_id) for input_id in ids_list)\n",
        "    max_seq_len = min(cur_max_seq_len, self.max_seq_len)\n",
        "    return max_seq_len\n",
        "  \n",
        "  @staticmethod\n",
        "  def pad_and_truncate(text_ids_list, max_seq_len):\n",
        "    input_ids_tensor = torch.zeros((len(text_ids_list), max_seq_len), dtype=torch.long)\n",
        "    \n",
        "    for i, text_ids in enumerate(text_ids_list):\n",
        "      seq_len = min(len(text_ids), max_seq_len)\n",
        "      input_ids_tensor[i, :seq_len] = torch.tensor(text_ids[:seq_len], dtype=torch.long)\n",
        "\n",
        "    return input_ids_tensor\n",
        "\n",
        "\n",
        "  def __call__(self, examples):\n",
        "    text_left_ids_list, text_right_ids_list, labels_list = list(zip(*examples))\n",
        "\n",
        "    max_text_left_length = self.get_max_seq_len(text_left_ids_list)\n",
        "    max_text_right_length = self.get_max_seq_len(text_right_ids_list)\n",
        "\n",
        "    text_left_ids = self.pad_and_truncate(text_left_ids_list, max_text_left_length)\n",
        "    text_right_ids = self.pad_and_truncate(text_right_ids_list, max_text_right_length)\n",
        "    labels = torch.tensor(labels_list, dtype=torch.long)\n",
        "\n",
        "    data_list = [text_left_ids, text_right_ids, labels]\n",
        "    return data_list\n",
        "                        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9iNenhoZCx4"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "from torch.utils.data import DataLoader\n",
        "def build_dataloader(train_df, test_df, config, token2id):\n",
        "  X_train, y_train, X_val, y_val, label2id, id2label = read_data(train_df, config['train_val_ratio'], token2id, mode='train')\n",
        "  X_test, y_test = read_data(test_df, config['train_val_ratio'], token2id, mode='test')\n",
        "\n",
        "  train_dataset = AFQMCDataset(X_train, y_train)\n",
        "  val_dataset = AFQMCDataset(X_val, y_val)\n",
        "  test_dataset = AFQMCDataset(X_test, y_test)\n",
        "\n",
        "  # ---------------------------------------------#\n",
        "  collate_fn = Collator(config['max_seq_len'])\n",
        "  # ---------------------------------------------#\n",
        "\n",
        "  train_dataloader = DataLoader(dataset=train_dataset, batch_size=config['batch_size'], num_workers=4, shuffle=True, collate_fn=collate_fn)\n",
        "  val_dataloader = DataLoader(dataset=val_dataset, batch_size=config['batch_size'], num_workers=4, shuffle=False, collate_fn=collate_fn)\n",
        "  test_dataloader = DataLoader(dataset=test_dataset, batch_size=config['batch_size'], num_workers=4, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "  return id2label, train_dataloader, val_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWCx-fXOZC0C",
        "outputId": "aba621ad-0e9a-4781-c818-653b3e91080c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preprocesing train data: 100%|██████████| 38650/38650 [00:12<00:00, 3058.09it/s]\n",
            "Preprocesing test data: 100%|██████████| 3861/3861 [00:01<00:00, 3082.16it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "id2label, train_dataloader, val_dataloader, test_dataloader = build_dataloader(train_df, test_df, config, token2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLdYRWNfZC2L",
        "outputId": "37e5a97e-6db8-4290-df91-7fb41c7951c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([[  16, 3195,   60,  ...,    0,    0,    0],\n",
            "        [  22, 3584, 2095,  ...,    0,    0,    0],\n",
            "        [  22,    5,  272,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 272, 1359, 3507,  ...,    0,    0,    0],\n",
            "        [  61, 2076, 1448,  ...,    0,    0,    0],\n",
            "        [  22, 1448, 1359,  ...,    0,    0,    0]]), tensor([[ 272, 1359,   60,  ...,    0,    0,    0],\n",
            "        [   1, 2947, 2036,  ...,    0,    0,    0],\n",
            "        [ 272, 1359, 1313,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 272, 1359,   22,  ...,    0,    0,    0],\n",
            "        [ 108,  713,  111,  ...,    0,    0,    0],\n",
            "        [ 803,   69, 4403,  ...,    0,    0,    0]]), tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
            "        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
            "        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0])]\n"
          ]
        }
      ],
      "source": [
        "for batch in train_dataloader:\n",
        "  print(batch)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtnpTzEkZC4l"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score,accuracy_score\n",
        "def evaluation(model, config, val_dataloader):\n",
        "  model.eval()\n",
        "  preds = []\n",
        "  labels = []\n",
        "  val_loss = 0.\n",
        "  val_iterator = tqdm(val_dataloader, desc='Evaluation', total=len(val_dataloader))\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in val_iterator:  \n",
        "      # ---------------------------------------------#\n",
        "      labels.append(batch[-1])\n",
        "      batch = [item.to(config['device']) for item in batch]\n",
        "      # ---------------------------------------------#\n",
        "\n",
        "      loss, logits = model(batch)[:2]\n",
        "      val_loss += loss.item()\n",
        "      \n",
        "      preds.append(logits.argmax(dim=-1).detach().cpu())\n",
        "\n",
        "  avg_val_loss = val_loss/len(val_dataloader)\n",
        "  labels = torch.cat(labels, dim=0).numpy()\n",
        "  preds = torch.cat(preds, dim=0).numpy()\n",
        "\n",
        "  f1 = f1_score(labels, preds, average='macro')\n",
        "  acc = accuracy_score(labels, preds)\n",
        "\n",
        "  return avg_val_loss, f1, acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqRzw6IfZC6u"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from tqdm import trange\n",
        "def train(model, config, train_dataloader, val_dataloader):\n",
        "  optimizer = AdamW(model.parameters(), lr=config['learning_rate'])\n",
        "  model.to(config['device'])\n",
        "  epoches_iterator = trange(config['num_epoches'])\n",
        "\n",
        "  global_steps = 0\n",
        "  train_loss = 0.\n",
        "  logging_loss = 0.\n",
        "\n",
        "  for epoch in epoches_iterator:\n",
        "    train_iterator = tqdm(train_dataloader, desc='Training', total=len(train_dataloader))\n",
        "    model.train()\n",
        "    \n",
        "    for batch in train_iterator:\n",
        "      # ---------------------------------------------#\n",
        "      batch = [item.to(config['device']) for item in batch]\n",
        "      loss = model(batch)[0]\n",
        "      # ---------------------------------------------#\n",
        "\n",
        "      model.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      train_loss += loss\n",
        "      global_steps +=1\n",
        "\n",
        "      if global_steps % config['logging_step'] == 0:\n",
        "        print_train_loss = (train_loss - logging_loss)/ config['logging_step'] \n",
        "        logging_loss = train_loss\n",
        "        avg_val_loss, f1, acc = evaluation(model, config, val_dataloader)\n",
        "        print(f'avg_val_loss:{avg_val_loss}, f1:{f1}, acc:{acc}')\n",
        "        model.train()\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWRAzQTrZC87"
      },
      "outputs": [],
      "source": [
        "def predict(config, id2label, model, test_dataloader):\n",
        "  model.eval()\n",
        "  test_iterator = tqdm(test_dataloader, desc='Predicting', total=len(test_dataloader))\n",
        "  test_preds =[]\n",
        "\n",
        "  with torch.no_grad():\n",
        "     for batch in test_iterator:\n",
        "       # ---------------------------------------------#\n",
        "       batch = [item.to(config['device']) for item in batch]\n",
        "       logits = model(batch)[1]\n",
        "       # ---------------------------------------------#\n",
        "       test_preds.append(logits.argmax(dim=-1).detach().cpu())\n",
        "  test_preds = torch.cat(test_preds, dim=0).numpy()\n",
        "  test_preds = [id2label[idx] for idx in test_preds]\n",
        "\n",
        "  test_df = pd.read_csv(config['test_file_path'], sep=',')\n",
        "  test_df['preds'] = test_preds\n",
        "  return test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VG_GMqqQo5s"
      },
      "outputs": [],
      "source": [
        "model_config = {\n",
        "    'embedding': emb_mat,\n",
        "    'freeze_emb': True,\n",
        "    'hidden_size': 256,\n",
        "    'dropout': 0.3,\n",
        "    'num_layers': 2,\n",
        "    'concat_layers': True,\n",
        "    'rnn_type': 'lstm',\n",
        "    'num_labels': len(id2label)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pK1GD5l7Qo79"
      },
      "outputs": [],
      "source": [
        "class RNNDropout(nn.Dropout):\n",
        "  # sequences_batch [B, L, D]\n",
        "  def forward(self, sequences_batch):\n",
        "    ones = sequences_batch.data.new_ones(sequences_batch.shape[0], sequences_batch.shape[-1])\n",
        "    # 随机 mask ones\n",
        "    dropout_mask = nn.functional.dropout(ones, self.p, self.training, inplace=False)\n",
        "\n",
        "    return dropout_mask.unsqueeze(1) * sequences_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqZX4A4MQo-X"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "class StackedBRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers, \n",
        "               dropout_rate=0, dropout_output=False, rnn_type=nn.LSTM, concat_layers=False):\n",
        "    super().__init__()\n",
        "    self.dropout_output = dropout_output\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.num_layers = num_layers\n",
        "    self.concat_layers = concat_layers\n",
        "    self.rnns = nn.ModuleList()\n",
        "    \n",
        "\n",
        "    for i in range(num_layers):\n",
        "      input_size = input_size if i ==0 else 2*hidden_size\n",
        "      self.rnns.append(rnn_type(input_size, hidden_size, num_layers=1, bidirectional=True))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x [B, L, D] -->[L, B, D]\n",
        "    x = x.transpose(0, 1)\n",
        "\n",
        "    outputs = [x]\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      rnn_input = outputs[-1]\n",
        "      if self.dropout_rate > 0:\n",
        "        rnn_input = F.dropout(rnn_input, p=self.drouput_rate, training=self.training)\n",
        "\n",
        "      # self.rnn[i](rnn_input) : (output, (h_n, c_n))\n",
        "      rnn_output = self.rnns[i](rnn_input)[0]\n",
        "      outputs.append(rnn_output)\n",
        "\n",
        "\n",
        "    # outputs [x, output0, output1]\n",
        "    if self.concat_layers:\n",
        "      output = torch.cat(outputs[1:],2)\n",
        "    else:\n",
        "      output = outputs[-1]\n",
        "    \n",
        "    # output [L, B, D] --> [B, L, D]\n",
        "    output = output.transpose(0,1)\n",
        "\n",
        "    if self.dropout_output and self.dropout_rate > 0:\n",
        "      output = F.dropout(output, p=self.dropout_rate, training=self.training)\n",
        "\n",
        "    return output.contiguous()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKjiCJY18Af-"
      },
      "outputs": [],
      "source": [
        "class BidirectionalAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # v1 [B, L, H]\n",
        "    # v1_mask [B, L]\n",
        "    # v2 [B, R, H]\n",
        "    # v2_mask [B, R]\n",
        "    \n",
        "  def forward(self, v1, v1_mask, v2, v2_mask):\n",
        "    # v1:b, v2:a\n",
        "    # similarity_matrix [B, L, R]\n",
        "    similarity_matrix = v1.bmm(v2.transpose(2, 1).contiguous())\n",
        "    # v2_v1_attn [B, L, R]\n",
        "    # v1_v2_attn [B, L, R]\n",
        "    v2_v1_attn = F.softmax(similarity_matrix.masked_fill(v1_mask.unsqueeze(2), -1e7), dim=1)\n",
        "    v1_v2_attn = F.softmax(similarity_matrix.masked_fill(v2_mask.unsqueeze(1), -1e7), dim=2)\n",
        "\n",
        "    # attented_v1 [B, L, R]@[B, R, H] ->[B, L, H]\n",
        "    attented_v1 = v1_v2_attn.bmm(v2)\n",
        "    # v2_v1_attn [B, L, R] ->[B, R, L]@[B, L, H] ->[B, R, H]\n",
        "    attented_v2 = v2_v1_attn.transpose(1,2).bmm(v1)\n",
        "\n",
        "    attented_v1.masked_fill(v1_mask.unsqueeze(2), 0)\n",
        "    attented_v2.masked_fill(v2_mask.unsqueeze(2), 0)\n",
        "    return attented_v1, attented_v2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSuJ1uHnQpAz"
      },
      "outputs": [],
      "source": [
        "class ESIM(nn.Module):\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    # ------------------------- input encoding -------------------------#\n",
        "    rnn_mapping = {'lstm': nn.LSTM, 'gru': nn.GRU}\n",
        "    self.embedding = nn.Embedding.from_pretrained(config['embedding'], freeze=config['freeze_emb'])\n",
        "\n",
        "    self.rnn_dropout = RNNDropout(p=config['dropout'])\n",
        "    rnn_size = config['hidden_size']\n",
        "\n",
        "    if config['concat_layers']:\n",
        "      rnn_size //=config['num_layers']\n",
        "\n",
        "    self.input_encoding = StackedBRNN(input_size=config['embedding'].size(1),\n",
        "                                      hidden_size=rnn_size//2,\n",
        "                                      num_layers=config['num_layers'],\n",
        "                                      rnn_type=rnn_mapping[config['rnn_type']],\n",
        "                                      concat_layers=config['concat_layers'])\n",
        "    # ------------------------- input encoding -------------------------#\n",
        "\n",
        "\n",
        "    # ------------------------- local inference collected over sequences -------------------------#\n",
        "    self.attention = BidirectionalAttention()\n",
        "    # ------------------------- local inference collected over sequences -------------------------#\n",
        "\n",
        "    # ------------------------- the composition layer -------------------------#\n",
        "    self.projection = nn.Sequential(\n",
        "        nn.Linear(4 * config['hidden_size'], config['hidden_size']),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.composition = StackedBRNN(input_size=config['hidden_size'],\n",
        "                                   hidden_size=rnn_size//2,\n",
        "                                   num_layers=config['num_layers'],\n",
        "                                   rnn_type=rnn_mapping[config['rnn_type']],\n",
        "                                   concat_layers=config['concat_layers'])\n",
        "    # ------------------------- the composition layer -------------------------#\n",
        "\n",
        "\n",
        "    self.classification = nn.Sequential(\n",
        "        nn.Dropout(p=config['dropout']),\n",
        "        nn.Linear(4 * config['hidden_size'], config['hidden_size']),\n",
        "        nn.Tanh(),\n",
        "        nn.Dropout(p=config['dropout'])\n",
        "    )\n",
        "\n",
        "    self.out = nn.Linear(config['hidden_size'], config['num_labels'])\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # ------------------------- input encoding -------------------------#\n",
        "    # query [B, L]\n",
        "    # doc [B, R]\n",
        "    query, doc = inputs[0].long(), inputs[1].long()\n",
        "\n",
        "    # query_mask [B, L]\n",
        "    # doc_mask [B, R]\n",
        "    # 将 pad 的部分标注为1\n",
        "    query_mask = (query == 0)\n",
        "    doc_mask = (doc == 0)\n",
        "\n",
        "    # query [B, L, D]\n",
        "    # doc [B, R, D]\n",
        "    query = self.embedding(query)\n",
        "    doc = self.embedding(doc)\n",
        "    \n",
        "    # query [B, L, D]\n",
        "    # doc [B, R, D]\n",
        "    query = self.rnn_dropout(query)\n",
        "    doc = self.rnn_dropout(doc)\n",
        "    \n",
        "    # query [B, L, H]\n",
        "    # doc [B, R, H]\n",
        "    query = self.input_encoding(query)\n",
        "    doc = self.input_encoding(doc)\n",
        "    # ------------------------- input encoding -------------------------#\n",
        "\n",
        "    # ------------------------- local inference collected over sequences -------------------------#\n",
        "    attened_query, attened_doc = self.attention(query, query_mask, doc, doc_mask)\n",
        "    # ------------------------- local inference collected over sequences -------------------------#\n",
        "\n",
        "\n",
        "    # ------------------------- enchancement of local inference information -------------------------#\n",
        "    # enchanced_query [B, L, 4*h]\n",
        "    # enchanced_doc [B, R, 4*h]\n",
        "    enchanced_query = torch.cat([query, attened_query, query - attened_query, query * attened_query], dim=-1)\n",
        "    enchanced_doc = torch.cat([doc, attened_doc, doc - attened_doc, doc * attened_doc], dim=-1)\n",
        "    # ------------------------- enchancement of local inference information -------------------------#\n",
        "\n",
        "    # ------------------------- the composition layer -------------------------#\n",
        "    # projected_query [B, L, H]\n",
        "    # projected_doc [B, R, H]\n",
        "    projected_query = self.projection(enchanced_query)\n",
        "    projected_doc = self.projection(enchanced_doc)\n",
        "\n",
        "    query = self.composition(projected_query)\n",
        "    doc = self.composition(projected_doc)\n",
        "    # ------------------------- the composition layer -------------------------#\n",
        "\n",
        "    # ------------------------- pooling -------------------------#    \n",
        "    # reverse_query_mask [B, L]\n",
        "    # reverse_doc_mask [B, R]\n",
        "    reverse_query_mask = 1. - query_mask.float()\n",
        "    reverse_doc_mask = 1. - doc_mask.float()\n",
        "\n",
        "    query_avg = torch.sum(query*reverse_query_mask.unsqueeze(2), dim=1)/ (torch.sum(reverse_query_mask, dim=1, keepdim=True) + 1e-8)\n",
        "    doc_avg = torch.sum(doc*reverse_doc_mask.unsqueeze(2), dim=1)/ (torch.sum(reverse_doc_mask, dim=1, keepdim=True) + 1e-8)\n",
        "    \n",
        "    # 确保max pooling时不会取到pad值\n",
        "    query = query.masked_fill(query_mask.unsqueeze(2), -1e7)\n",
        "    doc = doc.masked_fill(doc_mask.unsqueeze(2), -1e7)\n",
        "\n",
        "    query_max, _ = query.max(dim=1)\n",
        "    doc_max, _ = doc.max(dim=1)\n",
        "\n",
        "    # v [B, 4*H]\n",
        "    v = torch.cat([query_avg, query_max, doc_avg, doc_max], dim=-1)\n",
        "    # ------------------------- pooling -------------------------#  \n",
        "\n",
        "    # ------------------------- prediction -------------------------#  \n",
        "    # hidden [B, H]\n",
        "    hidden = self.classification(v)\n",
        "    out = self.out(hidden)\n",
        "    outputs = (out,)\n",
        "    # ------------------------- prediction -------------------------#     \n",
        "\n",
        "    if len(inputs) == 3:\n",
        "      loss_fct = nn.CrossEntropyLoss()\n",
        "      loss = loss_fct(out, inputs[-1])\n",
        "      outputs = (loss, ) + outputs\n",
        "\n",
        "    return outputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vO8V5cIUE3ej"
      },
      "outputs": [],
      "source": [
        "model = ESIM(model_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "011o3GXJE3gb",
        "outputId": "4f9dd68f-22ea-48da-8667-23480e1dc6cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "Training:   0%|          | 0/484 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "Training:   0%|          | 1/484 [00:00<07:42,  1.04it/s]\u001b[A\n",
            "Training:   0%|          | 2/484 [00:01<05:56,  1.35it/s]\u001b[A\n",
            "Training:   1%|          | 3/484 [00:02<05:14,  1.53it/s]\u001b[A\n",
            "Training:   1%|          | 4/484 [00:02<04:44,  1.69it/s]\u001b[A\n",
            "Training:   1%|          | 5/484 [00:03<04:17,  1.86it/s]\u001b[A\n",
            "Training:   1%|          | 6/484 [00:03<05:07,  1.55it/s]\u001b[A\n",
            "Training:   1%|▏         | 7/484 [00:04<04:54,  1.62it/s]\u001b[A\n",
            "Training:   2%|▏         | 8/484 [00:05<04:46,  1.66it/s]\u001b[A\n",
            "Training:   2%|▏         | 9/484 [00:05<04:34,  1.73it/s]\u001b[A\n",
            "Training:   2%|▏         | 10/484 [00:06<04:16,  1.85it/s]\u001b[A\n",
            "Training:   2%|▏         | 11/484 [00:06<04:34,  1.72it/s]\u001b[A\n",
            "Training:   2%|▏         | 12/484 [00:07<04:14,  1.86it/s]\u001b[A\n",
            "Training:   3%|▎         | 13/484 [00:07<04:45,  1.65it/s]\u001b[A\n",
            "Training:   3%|▎         | 14/484 [00:08<04:15,  1.84it/s]\u001b[A\n",
            "Training:   3%|▎         | 15/484 [00:08<03:59,  1.96it/s]\u001b[A\n",
            "Training:   3%|▎         | 16/484 [00:09<04:11,  1.86it/s]\u001b[A\n",
            "Training:   4%|▎         | 17/484 [00:10<04:36,  1.69it/s]\u001b[A\n",
            "Training:   4%|▎         | 18/484 [00:10<04:31,  1.72it/s]\u001b[A\n",
            "Training:   4%|▍         | 19/484 [00:11<05:28,  1.42it/s]\u001b[A\n",
            "Training:   4%|▍         | 20/484 [00:12<05:16,  1.47it/s]\u001b[A\n",
            "Training:   4%|▍         | 21/484 [00:12<05:07,  1.51it/s]\u001b[A\n",
            "Training:   5%|▍         | 22/484 [00:13<04:27,  1.73it/s]\u001b[A\n",
            "Training:   5%|▍         | 23/484 [00:13<04:12,  1.83it/s]\u001b[A\n",
            "Training:   5%|▍         | 24/484 [00:14<04:08,  1.85it/s]\u001b[A\n",
            "Training:   5%|▌         | 25/484 [00:14<04:10,  1.83it/s]\u001b[A\n",
            "Training:   5%|▌         | 26/484 [00:15<04:05,  1.87it/s]\u001b[A\n",
            "Training:   6%|▌         | 27/484 [00:15<04:11,  1.82it/s]\u001b[A\n",
            "Training:   6%|▌         | 28/484 [00:16<03:53,  1.95it/s]\u001b[A\n",
            "Training:   6%|▌         | 29/484 [00:17<04:49,  1.57it/s]\u001b[A\n",
            "Training:   6%|▌         | 30/484 [00:17<04:54,  1.54it/s]\u001b[A\n",
            "Training:   6%|▋         | 31/484 [00:18<04:20,  1.74it/s]\u001b[A\n",
            "Training:   7%|▋         | 32/484 [00:18<04:12,  1.79it/s]\u001b[A\n",
            "Training:   7%|▋         | 33/484 [00:19<04:06,  1.83it/s]\u001b[A\n",
            "Training:   7%|▋         | 34/484 [00:19<04:15,  1.76it/s]\u001b[A\n",
            "Training:   7%|▋         | 35/484 [00:20<04:19,  1.73it/s]\u001b[A\n",
            "Training:   7%|▋         | 36/484 [00:21<04:14,  1.76it/s]\u001b[A\n",
            "Training:   8%|▊         | 37/484 [00:21<03:51,  1.93it/s]\u001b[A\n",
            "Training:   8%|▊         | 38/484 [00:21<03:45,  1.98it/s]\u001b[A\n",
            "Training:   8%|▊         | 39/484 [00:22<03:58,  1.87it/s]\u001b[A\n",
            "Training:   8%|▊         | 40/484 [00:22<03:36,  2.05it/s]\u001b[A\n",
            "Training:   8%|▊         | 41/484 [00:23<03:24,  2.16it/s]\u001b[A\n",
            "Training:   9%|▊         | 42/484 [00:23<03:16,  2.25it/s]\u001b[A\n",
            "Training:   9%|▉         | 43/484 [00:24<04:19,  1.70it/s]\u001b[A\n",
            "Training:   9%|▉         | 44/484 [00:25<04:05,  1.79it/s]\u001b[A\n",
            "Training:   9%|▉         | 45/484 [00:25<03:57,  1.85it/s]\u001b[A\n",
            "Training:  10%|▉         | 46/484 [00:26<03:35,  2.03it/s]\u001b[A\n",
            "Training:  10%|▉         | 47/484 [00:26<03:27,  2.11it/s]\u001b[A\n",
            "Training:  10%|▉         | 48/484 [00:26<03:21,  2.17it/s]\u001b[A\n",
            "Training:  10%|█         | 49/484 [00:27<03:30,  2.06it/s]\u001b[A\n",
            "Training:  10%|█         | 50/484 [00:27<03:14,  2.23it/s]\u001b[A\n",
            "Training:  11%|█         | 51/484 [00:28<03:08,  2.30it/s]\u001b[A\n",
            "Training:  11%|█         | 52/484 [00:28<03:18,  2.17it/s]\u001b[A\n",
            "Training:  11%|█         | 53/484 [00:29<03:30,  2.05it/s]\u001b[A\n",
            "Training:  11%|█         | 54/484 [00:29<03:27,  2.07it/s]\u001b[A\n",
            "Training:  11%|█▏        | 55/484 [00:30<03:44,  1.91it/s]\u001b[A\n",
            "Training:  12%|█▏        | 56/484 [00:31<03:56,  1.81it/s]\u001b[A\n",
            "Training:  12%|█▏        | 57/484 [00:31<03:59,  1.78it/s]\u001b[A\n",
            "Training:  12%|█▏        | 58/484 [00:32<03:54,  1.82it/s]\u001b[A\n",
            "Training:  12%|█▏        | 59/484 [00:32<04:00,  1.77it/s]\u001b[A\n",
            "Training:  12%|█▏        | 60/484 [00:33<03:59,  1.77it/s]\u001b[A\n",
            "Training:  13%|█▎        | 61/484 [00:33<03:41,  1.91it/s]\u001b[A\n",
            "Training:  13%|█▎        | 62/484 [00:34<03:31,  1.99it/s]\u001b[A\n",
            "Training:  13%|█▎        | 63/484 [00:34<03:18,  2.12it/s]\u001b[A\n",
            "Training:  13%|█▎        | 64/484 [00:35<04:29,  1.56it/s]\u001b[A\n",
            "Training:  13%|█▎        | 65/484 [00:36<04:15,  1.64it/s]\u001b[A\n",
            "Training:  14%|█▎        | 66/484 [00:36<03:54,  1.78it/s]\u001b[A\n",
            "Training:  14%|█▍        | 67/484 [00:37<03:44,  1.86it/s]\u001b[A\n",
            "Training:  14%|█▍        | 68/484 [00:37<03:31,  1.96it/s]\u001b[A\n",
            "Training:  14%|█▍        | 69/484 [00:37<03:21,  2.06it/s]\u001b[A\n",
            "Training:  14%|█▍        | 70/484 [00:38<03:16,  2.11it/s]\u001b[A\n",
            "Training:  15%|█▍        | 71/484 [00:38<03:17,  2.09it/s]\u001b[A\n",
            "Training:  15%|█▍        | 72/484 [00:39<03:14,  2.12it/s]\u001b[A\n",
            "Training:  15%|█▌        | 73/484 [00:39<03:26,  1.99it/s]\u001b[A\n",
            "Training:  15%|█▌        | 74/484 [00:40<03:27,  1.98it/s]\u001b[A\n",
            "Training:  15%|█▌        | 75/484 [00:40<03:14,  2.10it/s]\u001b[A\n",
            "Training:  16%|█▌        | 76/484 [00:41<03:19,  2.05it/s]\u001b[A\n",
            "Training:  16%|█▌        | 77/484 [00:41<03:33,  1.91it/s]\u001b[A\n",
            "Training:  16%|█▌        | 78/484 [00:42<03:47,  1.78it/s]\u001b[A\n",
            "Training:  16%|█▋        | 79/484 [00:42<03:27,  1.95it/s]\u001b[A\n",
            "Training:  17%|█▋        | 80/484 [00:43<03:16,  2.06it/s]\u001b[A\n",
            "Training:  17%|█▋        | 81/484 [00:44<04:10,  1.61it/s]\u001b[A\n",
            "Training:  17%|█▋        | 82/484 [00:44<04:09,  1.61it/s]\u001b[A\n",
            "Training:  17%|█▋        | 83/484 [00:45<03:46,  1.77it/s]\u001b[A\n",
            "Training:  17%|█▋        | 84/484 [00:45<03:48,  1.75it/s]\u001b[A\n",
            "Training:  18%|█▊        | 85/484 [00:46<03:43,  1.79it/s]\u001b[A\n",
            "Training:  18%|█▊        | 86/484 [00:46<03:28,  1.91it/s]\u001b[A\n",
            "Training:  18%|█▊        | 87/484 [00:47<03:27,  1.91it/s]\u001b[A\n",
            "Training:  18%|█▊        | 88/484 [00:47<03:19,  1.99it/s]\u001b[A\n",
            "Training:  18%|█▊        | 89/484 [00:48<03:14,  2.03it/s]\u001b[A\n",
            "Training:  19%|█▊        | 90/484 [00:49<03:32,  1.86it/s]\u001b[A\n",
            "Training:  19%|█▉        | 91/484 [00:49<03:18,  1.98it/s]\u001b[A\n",
            "Training:  19%|█▉        | 92/484 [00:50<03:23,  1.92it/s]\u001b[A\n",
            "Training:  19%|█▉        | 93/484 [00:50<03:16,  1.99it/s]\u001b[A\n",
            "Training:  19%|█▉        | 94/484 [00:50<03:07,  2.08it/s]\u001b[A\n",
            "Training:  20%|█▉        | 95/484 [00:51<03:22,  1.92it/s]\u001b[A\n",
            "Training:  20%|█▉        | 96/484 [00:51<03:11,  2.03it/s]\u001b[A\n",
            "Training:  20%|██        | 97/484 [00:52<03:01,  2.13it/s]\u001b[A\n",
            "Training:  20%|██        | 98/484 [00:52<03:02,  2.12it/s]\u001b[A\n",
            "Training:  20%|██        | 99/484 [00:53<03:13,  1.99it/s]\u001b[A\n",
            "Training:  21%|██        | 100/484 [00:54<03:44,  1.71it/s]\u001b[A\n",
            "Training:  21%|██        | 101/484 [00:54<03:49,  1.67it/s]\u001b[A\n",
            "Training:  21%|██        | 102/484 [00:55<03:56,  1.61it/s]\u001b[A\n",
            "Training:  21%|██▏       | 103/484 [00:56<03:43,  1.70it/s]\u001b[A\n",
            "Training:  21%|██▏       | 104/484 [00:56<03:26,  1.84it/s]\u001b[A\n",
            "Training:  22%|██▏       | 105/484 [00:56<03:17,  1.92it/s]\u001b[A\n",
            "Training:  22%|██▏       | 106/484 [00:57<03:16,  1.92it/s]\u001b[A\n",
            "Training:  22%|██▏       | 107/484 [00:57<03:03,  2.05it/s]\u001b[A\n",
            "Training:  22%|██▏       | 108/484 [00:58<03:12,  1.95it/s]\u001b[A\n",
            "Training:  23%|██▎       | 109/484 [00:59<03:20,  1.87it/s]\u001b[A\n",
            "Training:  23%|██▎       | 110/484 [00:59<03:21,  1.86it/s]\u001b[A\n",
            "Training:  23%|██▎       | 111/484 [01:00<03:37,  1.71it/s]\u001b[A\n",
            "Training:  23%|██▎       | 112/484 [01:00<03:25,  1.81it/s]\u001b[A\n",
            "Training:  23%|██▎       | 113/484 [01:01<03:37,  1.70it/s]\u001b[A\n",
            "Training:  24%|██▎       | 114/484 [01:02<03:45,  1.64it/s]\u001b[A\n",
            "Training:  24%|██▍       | 115/484 [01:02<04:13,  1.45it/s]\u001b[A\n",
            "Training:  24%|██▍       | 116/484 [01:03<03:46,  1.63it/s]\u001b[A\n",
            "Training:  24%|██▍       | 117/484 [01:03<03:34,  1.71it/s]\u001b[A\n",
            "Training:  24%|██▍       | 118/484 [01:04<03:46,  1.61it/s]\u001b[A\n",
            "Training:  25%|██▍       | 119/484 [01:05<04:12,  1.44it/s]\u001b[A\n",
            "Training:  25%|██▍       | 120/484 [01:06<03:59,  1.52it/s]\u001b[A\n",
            "Training:  25%|██▌       | 121/484 [01:06<03:29,  1.73it/s]\u001b[A\n",
            "Training:  25%|██▌       | 122/484 [01:06<03:23,  1.78it/s]\u001b[A\n",
            "Training:  25%|██▌       | 123/484 [01:07<03:16,  1.84it/s]\u001b[A\n",
            "Training:  26%|██▌       | 124/484 [01:07<03:13,  1.86it/s]\u001b[A\n",
            "Training:  26%|██▌       | 125/484 [01:08<03:00,  1.99it/s]\u001b[A\n",
            "Training:  26%|██▌       | 126/484 [01:08<02:54,  2.05it/s]\u001b[A\n",
            "Training:  26%|██▌       | 127/484 [01:09<03:05,  1.92it/s]\u001b[A\n",
            "Training:  26%|██▋       | 128/484 [01:10<03:14,  1.83it/s]\u001b[A\n",
            "Training:  27%|██▋       | 129/484 [01:10<03:34,  1.66it/s]\u001b[A\n",
            "Training:  27%|██▋       | 130/484 [01:11<03:30,  1.68it/s]\u001b[A\n",
            "Training:  27%|██▋       | 131/484 [01:11<03:30,  1.68it/s]\u001b[A\n",
            "Training:  27%|██▋       | 132/484 [01:12<03:14,  1.81it/s]\u001b[A\n",
            "Training:  27%|██▋       | 133/484 [01:12<03:07,  1.88it/s]\u001b[A\n",
            "Training:  28%|██▊       | 134/484 [01:13<03:12,  1.82it/s]\u001b[A\n",
            "Training:  28%|██▊       | 135/484 [01:14<03:08,  1.85it/s]\u001b[A\n",
            "Training:  28%|██▊       | 136/484 [01:14<03:10,  1.83it/s]\u001b[A\n",
            "Training:  28%|██▊       | 137/484 [01:15<03:30,  1.64it/s]\u001b[A\n",
            "Training:  29%|██▊       | 138/484 [01:15<03:10,  1.82it/s]\u001b[A\n",
            "Training:  29%|██▊       | 139/484 [01:16<03:09,  1.82it/s]\u001b[A\n",
            "Training:  29%|██▉       | 140/484 [01:16<02:57,  1.94it/s]\u001b[A\n",
            "Training:  29%|██▉       | 141/484 [01:17<02:49,  2.02it/s]\u001b[A\n",
            "Training:  29%|██▉       | 142/484 [01:17<02:41,  2.12it/s]\u001b[A\n",
            "Training:  30%|██▉       | 143/484 [01:18<02:48,  2.03it/s]\u001b[A\n",
            "Training:  30%|██▉       | 144/484 [01:18<03:19,  1.70it/s]\u001b[A\n",
            "Training:  30%|██▉       | 145/484 [01:19<03:08,  1.80it/s]\u001b[A\n",
            "Training:  30%|███       | 146/484 [01:19<02:55,  1.93it/s]\u001b[A\n",
            "Training:  30%|███       | 147/484 [01:20<03:56,  1.43it/s]\u001b[A\n",
            "Training:  31%|███       | 148/484 [01:21<03:31,  1.59it/s]\u001b[A\n",
            "Training:  31%|███       | 149/484 [01:22<03:46,  1.48it/s]\u001b[A\n",
            "Training:  31%|███       | 150/484 [01:22<03:31,  1.58it/s]\u001b[A\n",
            "Training:  31%|███       | 151/484 [01:23<03:28,  1.60it/s]\u001b[A\n",
            "Training:  31%|███▏      | 152/484 [01:23<03:19,  1.66it/s]\u001b[A\n",
            "Training:  32%|███▏      | 153/484 [01:24<03:14,  1.70it/s]\u001b[A\n",
            "Training:  32%|███▏      | 154/484 [01:24<03:07,  1.76it/s]\u001b[A\n",
            "Training:  32%|███▏      | 155/484 [01:25<03:04,  1.78it/s]\u001b[A\n",
            "Training:  32%|███▏      | 156/484 [01:26<03:04,  1.78it/s]\u001b[A\n",
            "Training:  32%|███▏      | 157/484 [01:26<02:58,  1.83it/s]\u001b[A\n",
            "Training:  33%|███▎      | 158/484 [01:27<02:50,  1.91it/s]\u001b[A\n",
            "Training:  33%|███▎      | 159/484 [01:27<02:53,  1.87it/s]\u001b[A\n",
            "Training:  33%|███▎      | 160/484 [01:28<03:28,  1.55it/s]\u001b[A\n",
            "Training:  33%|███▎      | 161/484 [01:28<03:06,  1.73it/s]\u001b[A\n",
            "Training:  33%|███▎      | 162/484 [01:29<02:55,  1.84it/s]\u001b[A\n",
            "Training:  34%|███▎      | 163/484 [01:29<02:42,  1.98it/s]\u001b[A\n",
            "Training:  34%|███▍      | 164/484 [01:30<03:11,  1.67it/s]\u001b[A\n",
            "Training:  34%|███▍      | 165/484 [01:31<03:06,  1.71it/s]\u001b[A\n",
            "Training:  34%|███▍      | 166/484 [01:31<02:49,  1.87it/s]\u001b[A\n",
            "Training:  35%|███▍      | 167/484 [01:32<03:00,  1.76it/s]\u001b[A\n",
            "Training:  35%|███▍      | 168/484 [01:32<02:54,  1.81it/s]\u001b[A\n",
            "Training:  35%|███▍      | 169/484 [01:33<02:53,  1.81it/s]\u001b[A\n",
            "Training:  35%|███▌      | 170/484 [01:33<02:52,  1.82it/s]\u001b[A\n",
            "Training:  35%|███▌      | 171/484 [01:34<03:23,  1.54it/s]\u001b[A\n",
            "Training:  36%|███▌      | 172/484 [01:35<03:49,  1.36it/s]\u001b[A\n",
            "Training:  36%|███▌      | 173/484 [01:36<03:27,  1.50it/s]\u001b[A\n",
            "Training:  36%|███▌      | 174/484 [01:36<03:03,  1.69it/s]\u001b[A\n",
            "Training:  36%|███▌      | 175/484 [01:37<03:01,  1.70it/s]\u001b[A\n",
            "Training:  36%|███▋      | 176/484 [01:37<02:52,  1.78it/s]\u001b[A\n",
            "Training:  37%|███▋      | 177/484 [01:38<02:49,  1.81it/s]\u001b[A\n",
            "Training:  37%|███▋      | 178/484 [01:39<03:10,  1.61it/s]\u001b[A\n",
            "Training:  37%|███▋      | 179/484 [01:39<03:13,  1.58it/s]\u001b[A\n",
            "Training:  37%|███▋      | 180/484 [01:41<04:16,  1.18it/s]\u001b[A\n",
            "Training:  37%|███▋      | 181/484 [01:42<05:07,  1.01s/it]\u001b[A\n",
            "Training:  38%|███▊      | 182/484 [01:43<05:01,  1.00it/s]\u001b[A\n",
            "Training:  38%|███▊      | 183/484 [01:43<04:21,  1.15it/s]\u001b[A\n",
            "Training:  38%|███▊      | 184/484 [01:44<03:53,  1.29it/s]\u001b[A\n",
            "Training:  38%|███▊      | 185/484 [01:44<03:25,  1.45it/s]\u001b[A\n",
            "Training:  38%|███▊      | 186/484 [01:45<03:12,  1.54it/s]\u001b[A\n",
            "Training:  39%|███▊      | 187/484 [01:46<02:55,  1.69it/s]\u001b[A\n",
            "Training:  39%|███▉      | 188/484 [01:46<02:53,  1.70it/s]\u001b[A\n",
            "Training:  39%|███▉      | 189/484 [01:47<02:41,  1.82it/s]\u001b[A\n",
            "Training:  39%|███▉      | 190/484 [01:47<02:34,  1.91it/s]\u001b[A\n",
            "Training:  39%|███▉      | 191/484 [01:48<02:47,  1.75it/s]\u001b[A\n",
            "Training:  40%|███▉      | 192/484 [01:48<02:50,  1.71it/s]\u001b[A\n",
            "Training:  40%|███▉      | 193/484 [01:49<02:47,  1.74it/s]\u001b[A\n",
            "Training:  40%|████      | 194/484 [01:49<02:49,  1.72it/s]\u001b[A\n",
            "Training:  40%|████      | 195/484 [01:51<03:47,  1.27it/s]\u001b[A\n",
            "Training:  40%|████      | 196/484 [01:51<03:34,  1.34it/s]\u001b[A\n",
            "Training:  41%|████      | 197/484 [01:52<03:13,  1.49it/s]\u001b[A\n",
            "Training:  41%|████      | 198/484 [01:52<02:54,  1.64it/s]\u001b[A\n",
            "Training:  41%|████      | 199/484 [01:53<02:46,  1.71it/s]\u001b[A\n",
            "Training:  41%|████▏     | 200/484 [01:53<02:39,  1.78it/s]\u001b[A\n",
            "Training:  42%|████▏     | 201/484 [01:54<02:28,  1.91it/s]\u001b[A\n",
            "Training:  42%|████▏     | 202/484 [01:54<02:24,  1.95it/s]\u001b[A\n",
            "Training:  42%|████▏     | 203/484 [01:55<02:26,  1.92it/s]\u001b[A\n",
            "Training:  42%|████▏     | 204/484 [01:55<02:20,  1.99it/s]\u001b[A\n",
            "Training:  42%|████▏     | 205/484 [01:56<02:17,  2.03it/s]\u001b[A\n",
            "Training:  43%|████▎     | 206/484 [01:56<02:24,  1.92it/s]\u001b[A\n",
            "Training:  43%|████▎     | 207/484 [01:57<02:23,  1.93it/s]\u001b[A\n",
            "Training:  43%|████▎     | 208/484 [01:57<02:15,  2.04it/s]\u001b[A\n",
            "Training:  43%|████▎     | 209/484 [01:58<02:31,  1.81it/s]\u001b[A\n",
            "Training:  43%|████▎     | 210/484 [01:59<02:50,  1.60it/s]\u001b[A\n",
            "Training:  44%|████▎     | 211/484 [01:59<02:35,  1.75it/s]\u001b[A\n",
            "Training:  44%|████▍     | 212/484 [02:00<02:18,  1.96it/s]\u001b[A\n",
            "Training:  44%|████▍     | 213/484 [02:00<02:29,  1.81it/s]\u001b[A\n",
            "Training:  44%|████▍     | 214/484 [02:01<02:19,  1.94it/s]\u001b[A\n",
            "Training:  44%|████▍     | 215/484 [02:01<02:20,  1.91it/s]\u001b[A\n",
            "Training:  45%|████▍     | 216/484 [02:02<02:23,  1.87it/s]\u001b[A\n",
            "Training:  45%|████▍     | 217/484 [02:02<02:08,  2.07it/s]\u001b[A\n",
            "Training:  45%|████▌     | 218/484 [02:03<02:19,  1.91it/s]\u001b[A\n",
            "Training:  45%|████▌     | 219/484 [02:03<02:32,  1.73it/s]\u001b[A\n",
            "Training:  45%|████▌     | 220/484 [02:04<02:26,  1.80it/s]\u001b[A\n",
            "Training:  46%|████▌     | 221/484 [02:05<02:27,  1.78it/s]\u001b[A\n",
            "Training:  46%|████▌     | 222/484 [02:05<02:24,  1.81it/s]\u001b[A\n",
            "Training:  46%|████▌     | 223/484 [02:05<02:08,  2.03it/s]\u001b[A\n",
            "Training:  46%|████▋     | 224/484 [02:06<02:15,  1.92it/s]\u001b[A\n",
            "Training:  46%|████▋     | 225/484 [02:07<02:13,  1.94it/s]\u001b[A\n",
            "Training:  47%|████▋     | 226/484 [02:07<02:12,  1.94it/s]\u001b[A\n",
            "Training:  47%|████▋     | 227/484 [02:08<02:35,  1.65it/s]\u001b[A\n",
            "Training:  47%|████▋     | 228/484 [02:08<02:39,  1.60it/s]\u001b[A\n",
            "Training:  47%|████▋     | 229/484 [02:09<02:25,  1.76it/s]\u001b[A\n",
            "Training:  48%|████▊     | 230/484 [02:10<02:35,  1.63it/s]\u001b[A\n",
            "Training:  48%|████▊     | 231/484 [02:10<02:17,  1.84it/s]\u001b[A\n",
            "Training:  48%|████▊     | 232/484 [02:11<02:17,  1.83it/s]\u001b[A\n",
            "Training:  48%|████▊     | 233/484 [02:11<02:10,  1.92it/s]\u001b[A\n",
            "Training:  48%|████▊     | 234/484 [02:12<02:30,  1.66it/s]\u001b[A\n",
            "Training:  49%|████▊     | 235/484 [02:13<02:37,  1.58it/s]\u001b[A\n",
            "Training:  49%|████▉     | 236/484 [02:13<02:28,  1.67it/s]\u001b[A\n",
            "Training:  49%|████▉     | 237/484 [02:14<02:23,  1.72it/s]\u001b[A\n",
            "Training:  49%|████▉     | 238/484 [02:14<02:13,  1.84it/s]\u001b[A\n",
            "Training:  49%|████▉     | 239/484 [02:15<02:07,  1.92it/s]\u001b[A\n",
            "Training:  50%|████▉     | 240/484 [02:15<02:10,  1.87it/s]\u001b[A\n",
            "Training:  50%|████▉     | 241/484 [02:16<02:46,  1.46it/s]\u001b[A\n",
            "Training:  50%|█████     | 242/484 [02:17<02:26,  1.65it/s]\u001b[A\n",
            "Training:  50%|█████     | 243/484 [02:17<02:29,  1.61it/s]\u001b[A\n",
            "Training:  50%|█████     | 244/484 [02:18<02:16,  1.76it/s]\u001b[A\n",
            "Training:  51%|█████     | 245/484 [02:18<02:14,  1.77it/s]\u001b[A\n",
            "Training:  51%|█████     | 246/484 [02:19<02:07,  1.87it/s]\u001b[A\n",
            "Training:  51%|█████     | 247/484 [02:19<01:55,  2.05it/s]\u001b[A\n",
            "Training:  51%|█████     | 248/484 [02:19<01:50,  2.14it/s]\u001b[A\n",
            "Training:  51%|█████▏    | 249/484 [02:20<01:53,  2.06it/s]\u001b[A\n",
            "Training:  52%|█████▏    | 250/484 [02:20<01:51,  2.09it/s]\u001b[A\n",
            "Training:  52%|█████▏    | 251/484 [02:21<02:18,  1.69it/s]\u001b[A\n",
            "Training:  52%|█████▏    | 252/484 [02:22<02:09,  1.79it/s]\u001b[A\n",
            "Training:  52%|█████▏    | 253/484 [02:22<02:00,  1.92it/s]\u001b[A\n",
            "Training:  52%|█████▏    | 254/484 [02:23<01:54,  2.00it/s]\u001b[A\n",
            "Training:  53%|█████▎    | 255/484 [02:23<01:53,  2.03it/s]\u001b[A\n",
            "Training:  53%|█████▎    | 256/484 [02:24<01:52,  2.02it/s]\u001b[A\n",
            "Training:  53%|█████▎    | 257/484 [02:24<02:00,  1.88it/s]\u001b[A\n",
            "Training:  53%|█████▎    | 258/484 [02:25<02:05,  1.80it/s]\u001b[A\n",
            "Training:  54%|█████▎    | 259/484 [02:26<02:10,  1.73it/s]\u001b[A\n",
            "Training:  54%|█████▎    | 260/484 [02:26<02:14,  1.67it/s]\u001b[A\n",
            "Training:  54%|█████▍    | 261/484 [02:27<02:32,  1.46it/s]\u001b[A\n",
            "Training:  54%|█████▍    | 262/484 [02:27<02:11,  1.69it/s]\u001b[A\n",
            "Training:  54%|█████▍    | 263/484 [02:28<01:58,  1.86it/s]\u001b[A\n",
            "Training:  55%|█████▍    | 264/484 [02:28<02:01,  1.80it/s]\u001b[A\n",
            "Training:  55%|█████▍    | 265/484 [02:29<01:58,  1.85it/s]\u001b[A\n",
            "Training:  55%|█████▍    | 266/484 [02:29<01:50,  1.97it/s]\u001b[A\n",
            "Training:  55%|█████▌    | 267/484 [02:30<01:44,  2.08it/s]\u001b[A\n",
            "Training:  55%|█████▌    | 268/484 [02:30<01:41,  2.13it/s]\u001b[A\n",
            "Training:  56%|█████▌    | 269/484 [02:31<01:43,  2.08it/s]\u001b[A\n",
            "Training:  56%|█████▌    | 270/484 [02:31<01:43,  2.06it/s]\u001b[A\n",
            "Training:  56%|█████▌    | 271/484 [02:32<01:47,  1.98it/s]\u001b[A\n",
            "Training:  56%|█████▌    | 272/484 [02:32<01:45,  2.01it/s]\u001b[A\n",
            "Training:  56%|█████▋    | 273/484 [02:33<02:07,  1.65it/s]\u001b[A\n",
            "Training:  57%|█████▋    | 274/484 [02:33<01:51,  1.89it/s]\u001b[A\n",
            "Training:  57%|█████▋    | 275/484 [02:34<01:44,  2.00it/s]\u001b[A\n",
            "Training:  57%|█████▋    | 276/484 [02:34<01:41,  2.05it/s]\u001b[A\n",
            "Training:  57%|█████▋    | 277/484 [02:35<01:59,  1.74it/s]\u001b[A\n",
            "Training:  57%|█████▋    | 278/484 [02:36<01:45,  1.94it/s]\u001b[A\n",
            "Training:  58%|█████▊    | 279/484 [02:36<01:50,  1.86it/s]\u001b[A\n",
            "Training:  58%|█████▊    | 280/484 [02:37<01:43,  1.97it/s]\u001b[A\n",
            "Training:  58%|█████▊    | 281/484 [02:37<01:43,  1.96it/s]\u001b[A\n",
            "Training:  58%|█████▊    | 282/484 [02:38<01:55,  1.75it/s]\u001b[A\n",
            "Training:  58%|█████▊    | 283/484 [02:38<01:45,  1.91it/s]\u001b[A\n",
            "Training:  59%|█████▊    | 284/484 [02:39<01:39,  2.01it/s]\u001b[A\n",
            "Training:  59%|█████▉    | 285/484 [02:39<01:32,  2.15it/s]\u001b[A\n",
            "Training:  59%|█████▉    | 286/484 [02:39<01:32,  2.14it/s]\u001b[A\n",
            "Training:  59%|█████▉    | 287/484 [02:40<01:35,  2.05it/s]\u001b[A\n",
            "Training:  60%|█████▉    | 288/484 [02:41<01:54,  1.71it/s]\u001b[A\n",
            "Training:  60%|█████▉    | 289/484 [02:42<01:59,  1.63it/s]\u001b[A\n",
            "Training:  60%|█████▉    | 290/484 [02:42<01:45,  1.84it/s]\u001b[A\n",
            "Training:  60%|██████    | 291/484 [02:42<01:40,  1.92it/s]\u001b[A\n",
            "Training:  60%|██████    | 292/484 [02:43<01:33,  2.05it/s]\u001b[A\n",
            "Training:  61%|██████    | 293/484 [02:43<01:42,  1.86it/s]\u001b[A\n",
            "Training:  61%|██████    | 294/484 [02:44<01:41,  1.86it/s]\u001b[A\n",
            "Training:  61%|██████    | 295/484 [02:44<01:37,  1.94it/s]\u001b[A\n",
            "Training:  61%|██████    | 296/484 [02:45<01:35,  1.96it/s]\u001b[A\n",
            "Training:  61%|██████▏   | 297/484 [02:45<01:34,  1.98it/s]\u001b[A\n",
            "Training:  62%|██████▏   | 298/484 [02:46<01:31,  2.03it/s]\u001b[A\n",
            "Training:  62%|██████▏   | 299/484 [02:47<01:48,  1.70it/s]\u001b[A\n",
            "\n",
            "Evaluation:   0%|          | 0/121 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   1%|          | 1/121 [00:00<01:05,  1.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   2%|▏         | 2/121 [00:00<00:41,  2.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   2%|▏         | 3/121 [00:01<00:36,  3.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   3%|▎         | 4/121 [00:01<00:32,  3.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   4%|▍         | 5/121 [00:01<00:38,  3.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   5%|▍         | 6/121 [00:01<00:36,  3.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   6%|▌         | 7/121 [00:02<00:35,  3.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   7%|▋         | 8/121 [00:02<00:31,  3.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   7%|▋         | 9/121 [00:02<00:26,  4.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   8%|▊         | 10/121 [00:02<00:26,  4.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:   9%|▉         | 11/121 [00:03<00:24,  4.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  10%|▉         | 12/121 [00:03<00:22,  4.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  11%|█         | 13/121 [00:03<00:21,  5.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  12%|█▏        | 14/121 [00:03<00:26,  4.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  12%|█▏        | 15/121 [00:03<00:24,  4.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  13%|█▎        | 16/121 [00:04<00:22,  4.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  14%|█▍        | 17/121 [00:04<00:20,  5.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  15%|█▍        | 18/121 [00:04<00:22,  4.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  16%|█▌        | 19/121 [00:04<00:21,  4.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  17%|█▋        | 20/121 [00:04<00:21,  4.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  17%|█▋        | 21/121 [00:05<00:19,  5.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  18%|█▊        | 22/121 [00:05<00:19,  5.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  19%|█▉        | 23/121 [00:05<00:25,  3.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  20%|█▉        | 24/121 [00:05<00:24,  4.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  21%|██        | 25/121 [00:06<00:21,  4.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  21%|██▏       | 26/121 [00:06<00:21,  4.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  22%|██▏       | 27/121 [00:06<00:20,  4.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  23%|██▎       | 28/121 [00:06<00:18,  4.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  24%|██▍       | 29/121 [00:06<00:17,  5.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  25%|██▍       | 30/121 [00:07<00:17,  5.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  26%|██▌       | 31/121 [00:07<00:18,  4.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  26%|██▋       | 32/121 [00:07<00:17,  5.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  27%|██▋       | 33/121 [00:07<00:16,  5.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  28%|██▊       | 34/121 [00:07<00:18,  4.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  29%|██▉       | 35/121 [00:08<00:17,  4.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  30%|██▉       | 36/121 [00:08<00:15,  5.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  31%|███       | 37/121 [00:08<00:15,  5.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  31%|███▏      | 38/121 [00:08<00:14,  5.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  32%|███▏      | 39/121 [00:08<00:15,  5.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  33%|███▎      | 40/121 [00:09<00:17,  4.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  34%|███▍      | 41/121 [00:09<00:15,  5.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  35%|███▍      | 42/121 [00:09<00:15,  5.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  36%|███▌      | 43/121 [00:09<00:14,  5.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  36%|███▋      | 44/121 [00:09<00:14,  5.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  37%|███▋      | 45/121 [00:10<00:14,  5.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  38%|███▊      | 46/121 [00:10<00:14,  5.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  39%|███▉      | 47/121 [00:10<00:14,  5.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  40%|███▉      | 48/121 [00:10<00:15,  4.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  40%|████      | 49/121 [00:10<00:14,  5.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  41%|████▏     | 50/121 [00:10<00:13,  5.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  42%|████▏     | 51/121 [00:11<00:13,  5.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  43%|████▎     | 52/121 [00:11<00:14,  4.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  44%|████▍     | 53/121 [00:11<00:13,  4.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  45%|████▍     | 54/121 [00:11<00:13,  4.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  45%|████▌     | 55/121 [00:12<00:12,  5.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  46%|████▋     | 56/121 [00:12<00:12,  5.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  47%|████▋     | 57/121 [00:12<00:12,  5.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  48%|████▊     | 58/121 [00:12<00:13,  4.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  49%|████▉     | 59/121 [00:12<00:12,  5.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  50%|████▉     | 60/121 [00:13<00:15,  3.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  50%|█████     | 61/121 [00:13<00:13,  4.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  51%|█████     | 62/121 [00:13<00:12,  4.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  52%|█████▏    | 63/121 [00:13<00:11,  4.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  53%|█████▎    | 64/121 [00:13<00:10,  5.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  54%|█████▎    | 65/121 [00:14<00:10,  5.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  55%|█████▍    | 66/121 [00:14<00:11,  4.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  55%|█████▌    | 67/121 [00:14<00:10,  5.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  56%|█████▌    | 68/121 [00:14<00:11,  4.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  57%|█████▋    | 69/121 [00:14<00:10,  4.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  58%|█████▊    | 70/121 [00:15<00:10,  5.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  59%|█████▊    | 71/121 [00:15<00:09,  5.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  60%|█████▉    | 72/121 [00:15<00:11,  4.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  60%|██████    | 73/121 [00:15<00:10,  4.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  61%|██████    | 74/121 [00:16<00:10,  4.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  62%|██████▏   | 75/121 [00:16<00:10,  4.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  63%|██████▎   | 76/121 [00:16<00:09,  4.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  64%|██████▎   | 77/121 [00:16<00:08,  5.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  64%|██████▍   | 78/121 [00:16<00:07,  5.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  65%|██████▌   | 79/121 [00:17<00:08,  4.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  66%|██████▌   | 80/121 [00:17<00:08,  5.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  67%|██████▋   | 81/121 [00:17<00:08,  4.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  68%|██████▊   | 82/121 [00:17<00:08,  4.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  69%|██████▊   | 83/121 [00:17<00:07,  4.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  69%|██████▉   | 84/121 [00:18<00:07,  5.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  70%|███████   | 85/121 [00:18<00:06,  5.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  71%|███████   | 86/121 [00:18<00:07,  4.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  72%|███████▏  | 87/121 [00:18<00:06,  4.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  73%|███████▎  | 88/121 [00:18<00:06,  5.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  74%|███████▎  | 89/121 [00:18<00:05,  5.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  74%|███████▍  | 90/121 [00:19<00:05,  5.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  75%|███████▌  | 91/121 [00:19<00:05,  5.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  76%|███████▌  | 92/121 [00:19<00:05,  5.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  77%|███████▋  | 93/121 [00:19<00:05,  5.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  78%|███████▊  | 94/121 [00:19<00:04,  5.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  79%|███████▊  | 95/121 [00:20<00:04,  5.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  79%|███████▉  | 96/121 [00:20<00:05,  4.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  80%|████████  | 97/121 [00:20<00:04,  4.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  81%|████████  | 98/121 [00:20<00:05,  4.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  82%|████████▏ | 99/121 [00:21<00:05,  4.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  83%|████████▎ | 100/121 [00:21<00:04,  4.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  83%|████████▎ | 101/121 [00:21<00:04,  4.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  84%|████████▍ | 102/121 [00:21<00:04,  4.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  85%|████████▌ | 103/121 [00:21<00:03,  4.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  86%|████████▌ | 104/121 [00:22<00:03,  4.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  87%|████████▋ | 105/121 [00:22<00:03,  4.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  88%|████████▊ | 106/121 [00:22<00:03,  4.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  88%|████████▊ | 107/121 [00:22<00:03,  4.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  89%|████████▉ | 108/121 [00:23<00:02,  4.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  90%|█████████ | 109/121 [00:23<00:02,  4.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  91%|█████████ | 110/121 [00:23<00:02,  4.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  92%|█████████▏| 111/121 [00:23<00:02,  4.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  93%|█████████▎| 112/121 [00:23<00:01,  5.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  93%|█████████▎| 113/121 [00:24<00:01,  4.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  94%|█████████▍| 114/121 [00:24<00:01,  5.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  95%|█████████▌| 115/121 [00:24<00:01,  5.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  96%|█████████▌| 116/121 [00:24<00:00,  5.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  97%|█████████▋| 117/121 [00:24<00:00,  5.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  98%|█████████▊| 118/121 [00:24<00:00,  5.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  98%|█████████▊| 119/121 [00:25<00:00,  5.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation:  99%|█████████▉| 120/121 [00:25<00:00,  5.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluation: 100%|██████████| 121/121 [00:25<00:00,  4.72it/s]\n",
            "\n",
            "Training:  62%|██████▏   | 300/484 [03:13<25:19,  8.26s/it]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6384318919221231 0.39919166796207056\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Training:  62%|██████▏   | 301/484 [03:14<18:14,  5.98s/it]\u001b[A\n",
            "Training:  62%|██████▏   | 302/484 [03:14<13:11,  4.35s/it]\u001b[A\n",
            "Training:  63%|██████▎   | 303/484 [03:15<09:35,  3.18s/it]\u001b[A\n",
            "Training:  63%|██████▎   | 304/484 [03:15<07:05,  2.36s/it]\u001b[A\n",
            "Training:  63%|██████▎   | 305/484 [03:15<05:18,  1.78s/it]\u001b[A\n",
            "Training:  63%|██████▎   | 306/484 [03:16<04:09,  1.40s/it]\u001b[A\n",
            "Training:  63%|██████▎   | 307/484 [03:17<03:27,  1.17s/it]\u001b[A\n",
            "Training:  64%|██████▎   | 308/484 [03:17<02:53,  1.02it/s]\u001b[A\n",
            "Training:  64%|██████▍   | 309/484 [03:17<02:18,  1.26it/s]\u001b[A\n",
            "Training:  64%|██████▍   | 310/484 [03:18<01:57,  1.48it/s]\u001b[A\n",
            "Training:  64%|██████▍   | 311/484 [03:19<01:57,  1.47it/s]\u001b[A\n",
            "Training:  64%|██████▍   | 312/484 [03:19<01:53,  1.52it/s]\u001b[A\n",
            "Training:  65%|██████▍   | 313/484 [03:20<01:47,  1.59it/s]\u001b[A\n",
            "Training:  65%|██████▍   | 314/484 [03:20<01:37,  1.75it/s]\u001b[A\n",
            "Training:  65%|██████▌   | 315/484 [03:21<01:39,  1.69it/s]\u001b[A\n",
            "Training:  65%|██████▌   | 316/484 [03:21<01:29,  1.89it/s]\u001b[A\n",
            "Training:  65%|██████▌   | 317/484 [03:22<01:40,  1.66it/s]\u001b[A\n",
            "Training:  66%|██████▌   | 318/484 [03:22<01:35,  1.73it/s]\u001b[A\n",
            "Training:  66%|██████▌   | 319/484 [03:23<01:31,  1.80it/s]\u001b[A\n",
            "Training:  66%|██████▌   | 320/484 [03:23<01:25,  1.92it/s]\u001b[A\n",
            "Training:  66%|██████▋   | 321/484 [03:24<01:28,  1.85it/s]\u001b[A\n",
            "Training:  67%|██████▋   | 322/484 [03:25<01:27,  1.85it/s]\u001b[A\n",
            "Training:  67%|██████▋   | 323/484 [03:25<01:22,  1.95it/s]\u001b[A\n",
            "Training:  67%|██████▋   | 324/484 [03:25<01:21,  1.97it/s]\u001b[A\n",
            "Training:  67%|██████▋   | 325/484 [03:26<01:39,  1.60it/s]\u001b[A\n",
            "Training:  67%|██████▋   | 326/484 [03:27<01:42,  1.54it/s]\u001b[A\n",
            "Training:  68%|██████▊   | 327/484 [03:28<01:33,  1.68it/s]\u001b[A\n",
            "Training:  68%|██████▊   | 328/484 [03:28<01:29,  1.74it/s]\u001b[A\n",
            "Training:  68%|██████▊   | 329/484 [03:28<01:21,  1.90it/s]\u001b[A\n",
            "Training:  68%|██████▊   | 330/484 [03:29<01:28,  1.74it/s]\u001b[A\n",
            "Training:  68%|██████▊   | 331/484 [03:30<01:23,  1.84it/s]\u001b[A\n",
            "Training:  69%|██████▊   | 332/484 [03:30<01:28,  1.73it/s]\u001b[A\n",
            "Training:  69%|██████▉   | 333/484 [03:31<01:23,  1.82it/s]\u001b[A\n",
            "Training:  69%|██████▉   | 334/484 [03:32<01:41,  1.47it/s]\u001b[A\n",
            "Training:  69%|██████▉   | 335/484 [03:32<01:28,  1.68it/s]\u001b[A\n",
            "Training:  69%|██████▉   | 336/484 [03:33<01:29,  1.65it/s]\u001b[A\n",
            "Training:  70%|██████▉   | 337/484 [03:33<01:23,  1.76it/s]\u001b[A\n",
            "Training:  70%|██████▉   | 338/484 [03:34<01:23,  1.76it/s]\u001b[A\n",
            "Training:  70%|███████   | 339/484 [03:34<01:17,  1.88it/s]\u001b[A\n",
            "Training:  70%|███████   | 340/484 [03:35<01:13,  1.96it/s]\u001b[A\n",
            "Training:  70%|███████   | 341/484 [03:35<01:09,  2.07it/s]\u001b[A\n",
            "Training:  71%|███████   | 342/484 [03:36<01:18,  1.81it/s]\u001b[A\n",
            "Training:  71%|███████   | 343/484 [03:36<01:14,  1.90it/s]\u001b[A\n",
            "Training:  71%|███████   | 344/484 [03:37<01:13,  1.91it/s]\u001b[A\n",
            "Training:  71%|███████▏  | 345/484 [03:37<01:04,  2.16it/s]\u001b[A\n",
            "Training:  71%|███████▏  | 346/484 [03:38<01:01,  2.23it/s]\u001b[A\n",
            "Training:  72%|███████▏  | 347/484 [03:38<01:02,  2.20it/s]\u001b[A\n",
            "Training:  72%|███████▏  | 348/484 [03:39<01:03,  2.15it/s]\u001b[A\n",
            "Training:  72%|███████▏  | 349/484 [03:39<01:01,  2.19it/s]\u001b[A\n",
            "Training:  72%|███████▏  | 350/484 [03:39<00:57,  2.32it/s]\u001b[A\n",
            "Training:  73%|███████▎  | 351/484 [03:40<00:57,  2.33it/s]\u001b[A\n",
            "Training:  73%|███████▎  | 352/484 [03:40<00:57,  2.29it/s]\u001b[A\n",
            "Training:  73%|███████▎  | 353/484 [03:41<00:59,  2.20it/s]\u001b[A\n",
            "Training:  73%|███████▎  | 354/484 [03:41<01:00,  2.14it/s]\u001b[A\n",
            "Training:  73%|███████▎  | 355/484 [03:42<01:06,  1.94it/s]\u001b[A\n",
            "Training:  74%|███████▎  | 356/484 [03:42<01:06,  1.94it/s]\u001b[A\n",
            "Training:  74%|███████▍  | 357/484 [03:43<01:00,  2.09it/s]\u001b[A\n",
            "Training:  74%|███████▍  | 358/484 [03:43<01:00,  2.09it/s]\u001b[A\n",
            "Training:  74%|███████▍  | 359/484 [03:44<01:00,  2.05it/s]\u001b[A\n",
            "Training:  74%|███████▍  | 360/484 [03:44<01:05,  1.91it/s]\u001b[A\n",
            "Training:  75%|███████▍  | 361/484 [03:45<01:07,  1.84it/s]\u001b[A\n",
            "Training:  75%|███████▍  | 362/484 [03:46<01:18,  1.55it/s]\u001b[A\n",
            "Training:  75%|███████▌  | 363/484 [03:46<01:14,  1.62it/s]\u001b[A\n",
            "Training:  75%|███████▌  | 364/484 [03:47<01:18,  1.52it/s]\u001b[A\n",
            "Training:  75%|███████▌  | 365/484 [03:48<01:22,  1.44it/s]\u001b[A\n",
            "Training:  76%|███████▌  | 366/484 [03:48<01:17,  1.53it/s]\u001b[A\n",
            "Training:  76%|███████▌  | 367/484 [03:49<01:10,  1.66it/s]\u001b[A\n",
            "Training:  76%|███████▌  | 368/484 [03:49<01:06,  1.74it/s]\u001b[A\n",
            "Training:  76%|███████▌  | 369/484 [03:50<01:11,  1.61it/s]\u001b[A\n",
            "Training:  76%|███████▋  | 370/484 [03:51<01:13,  1.55it/s]\u001b[A\n",
            "Training:  77%|███████▋  | 371/484 [03:52<01:26,  1.31it/s]\u001b[A\n",
            "Training:  77%|███████▋  | 372/484 [03:53<01:23,  1.34it/s]\u001b[A\n",
            "Training:  77%|███████▋  | 373/484 [03:53<01:19,  1.40it/s]\u001b[A\n",
            "Training:  77%|███████▋  | 374/484 [03:54<01:10,  1.56it/s]\u001b[A\n",
            "Training:  77%|███████▋  | 375/484 [03:54<01:06,  1.64it/s]\u001b[A\n",
            "Training:  78%|███████▊  | 376/484 [03:55<01:05,  1.64it/s]\u001b[A\n",
            "Training:  78%|███████▊  | 377/484 [03:55<00:59,  1.80it/s]\u001b[A\n",
            "Training:  78%|███████▊  | 378/484 [03:56<00:58,  1.80it/s]\u001b[A\n",
            "Training:  78%|███████▊  | 379/484 [03:56<00:57,  1.82it/s]\u001b[A\n",
            "Training:  79%|███████▊  | 380/484 [03:57<00:56,  1.83it/s]\u001b[A\n",
            "Training:  79%|███████▊  | 381/484 [03:58<00:56,  1.81it/s]\u001b[A\n",
            "Training:  79%|███████▉  | 382/484 [03:58<00:55,  1.83it/s]\u001b[A\n",
            "Training:  79%|███████▉  | 383/484 [03:59<00:55,  1.81it/s]\u001b[A\n",
            "Training:  79%|███████▉  | 384/484 [03:59<00:49,  2.01it/s]\u001b[A\n",
            "Training:  80%|███████▉  | 385/484 [04:00<00:50,  1.96it/s]\u001b[A\n",
            "Training:  80%|███████▉  | 386/484 [04:00<00:48,  2.01it/s]\u001b[A\n",
            "Training:  80%|███████▉  | 387/484 [04:00<00:44,  2.20it/s]\u001b[A\n",
            "Training:  80%|████████  | 388/484 [04:01<00:52,  1.83it/s]\u001b[A\n",
            "Training:  80%|████████  | 389/484 [04:02<00:51,  1.85it/s]\u001b[A\n",
            "Training:  81%|████████  | 390/484 [04:02<00:48,  1.93it/s]\u001b[A\n",
            "Training:  81%|████████  | 391/484 [04:03<00:47,  1.98it/s]\u001b[A\n",
            "Training:  81%|████████  | 392/484 [04:03<00:47,  1.95it/s]\u001b[A\n",
            "Training:  81%|████████  | 393/484 [04:04<00:43,  2.08it/s]\u001b[A\n",
            "Training:  81%|████████▏ | 394/484 [04:04<00:44,  2.03it/s]\u001b[A\n",
            "Training:  82%|████████▏ | 395/484 [04:04<00:42,  2.12it/s]\u001b[A\n",
            "Training:  82%|████████▏ | 396/484 [04:05<00:47,  1.86it/s]\u001b[A\n",
            "Training:  82%|████████▏ | 397/484 [04:06<00:44,  1.94it/s]\u001b[A\n",
            "Training:  82%|████████▏ | 398/484 [04:06<00:42,  2.01it/s]\u001b[A\n",
            "Training:  82%|████████▏ | 399/484 [04:07<00:42,  2.00it/s]\u001b[A\n",
            "Training:  83%|████████▎ | 400/484 [04:07<00:41,  2.00it/s]\u001b[A\n",
            "Training:  83%|████████▎ | 401/484 [04:08<00:39,  2.10it/s]\u001b[A\n",
            "Training:  83%|████████▎ | 402/484 [04:08<00:37,  2.19it/s]\u001b[A\n",
            "Training:  83%|████████▎ | 403/484 [04:08<00:37,  2.18it/s]\u001b[A\n",
            "Training:  83%|████████▎ | 404/484 [04:09<00:39,  2.05it/s]\u001b[A\n",
            "Training:  84%|████████▎ | 405/484 [04:09<00:39,  2.01it/s]\u001b[A\n",
            "Training:  84%|████████▍ | 406/484 [04:10<00:49,  1.58it/s]\u001b[A\n",
            "Training:  84%|████████▍ | 407/484 [04:11<00:49,  1.56it/s]\u001b[A\n",
            "Training:  84%|████████▍ | 408/484 [04:12<00:55,  1.38it/s]\u001b[A\n",
            "Training:  85%|████████▍ | 409/484 [04:12<00:46,  1.60it/s]\u001b[A\n",
            "Training:  85%|████████▍ | 410/484 [04:13<00:43,  1.71it/s]\u001b[A\n",
            "Training:  85%|████████▍ | 411/484 [04:13<00:41,  1.75it/s]\u001b[A\n",
            "Training:  85%|████████▌ | 412/484 [04:14<00:38,  1.85it/s]\u001b[A\n",
            "Training:  85%|████████▌ | 413/484 [04:14<00:37,  1.87it/s]\u001b[A\n",
            "Training:  86%|████████▌ | 414/484 [04:15<00:37,  1.86it/s]\u001b[A\n",
            "Training:  86%|████████▌ | 415/484 [04:16<00:39,  1.75it/s]\u001b[A\n",
            "Training:  86%|████████▌ | 416/484 [04:16<00:38,  1.78it/s]\u001b[A\n",
            "Training:  86%|████████▌ | 417/484 [04:17<00:35,  1.87it/s]\u001b[A\n",
            "Training:  86%|████████▋ | 418/484 [04:17<00:41,  1.60it/s]\u001b[A\n",
            "Training:  87%|████████▋ | 419/484 [04:18<00:37,  1.73it/s]\u001b[A\n",
            "Training:  87%|████████▋ | 420/484 [04:18<00:35,  1.79it/s]\u001b[A\n",
            "Training:  87%|████████▋ | 421/484 [04:19<00:33,  1.90it/s]\u001b[A\n",
            "Training:  87%|████████▋ | 422/484 [04:19<00:30,  2.02it/s]\u001b[A\n",
            "Training:  87%|████████▋ | 423/484 [04:20<00:27,  2.19it/s]\u001b[A\n",
            "Training:  88%|████████▊ | 424/484 [04:20<00:28,  2.07it/s]\u001b[A\n",
            "Training:  88%|████████▊ | 425/484 [04:21<00:29,  2.02it/s]\u001b[A\n",
            "Training:  88%|████████▊ | 426/484 [04:21<00:27,  2.14it/s]\u001b[A\n",
            "Training:  88%|████████▊ | 427/484 [04:22<00:28,  2.02it/s]\u001b[A\n",
            "Training:  88%|████████▊ | 428/484 [04:22<00:29,  1.87it/s]\u001b[A\n",
            "Training:  89%|████████▊ | 429/484 [04:23<00:30,  1.81it/s]\u001b[A\n",
            "Training:  89%|████████▉ | 430/484 [04:23<00:26,  2.01it/s]\u001b[A\n",
            "Training:  89%|████████▉ | 431/484 [04:24<00:28,  1.86it/s]\u001b[A\n",
            "Training:  89%|████████▉ | 432/484 [04:24<00:26,  1.95it/s]\u001b[A\n",
            "Training:  89%|████████▉ | 433/484 [04:25<00:24,  2.05it/s]\u001b[A\n",
            "Training:  90%|████████▉ | 434/484 [04:25<00:22,  2.19it/s]\u001b[A\n",
            "Training:  90%|████████▉ | 435/484 [04:26<00:21,  2.25it/s]\u001b[A\n",
            "Training:  90%|█████████ | 436/484 [04:26<00:20,  2.30it/s]\u001b[A\n",
            "Training:  90%|█████████ | 437/484 [04:27<00:23,  2.04it/s]\u001b[A\n",
            "Training:  90%|█████████ | 438/484 [04:27<00:23,  1.99it/s]\u001b[A\n",
            "Training:  91%|█████████ | 439/484 [04:28<00:22,  2.02it/s]\u001b[A\n",
            "Training:  91%|█████████ | 440/484 [04:28<00:22,  2.00it/s]\u001b[A\n",
            "Training:  91%|█████████ | 441/484 [04:29<00:24,  1.74it/s]\u001b[A\n",
            "Training:  91%|█████████▏| 442/484 [04:29<00:23,  1.79it/s]\u001b[A\n",
            "Training:  92%|█████████▏| 443/484 [04:30<00:23,  1.72it/s]\u001b[A\n",
            "Training:  92%|█████████▏| 444/484 [04:30<00:21,  1.89it/s]\u001b[A\n",
            "Training:  92%|█████████▏| 445/484 [04:31<00:20,  1.89it/s]\u001b[A\n",
            "Training:  92%|█████████▏| 446/484 [04:31<00:17,  2.13it/s]\u001b[A\n",
            "Training:  92%|█████████▏| 447/484 [04:32<00:18,  1.99it/s]\u001b[A\n",
            "Training:  93%|█████████▎| 448/484 [04:32<00:17,  2.06it/s]\u001b[A\n",
            "Training:  93%|█████████▎| 449/484 [04:33<00:15,  2.25it/s]\u001b[A\n",
            "Training:  93%|█████████▎| 450/484 [04:34<00:18,  1.80it/s]\u001b[A\n",
            "Training:  93%|█████████▎| 451/484 [04:34<00:17,  1.84it/s]\u001b[A\n",
            "Training:  93%|█████████▎| 452/484 [04:34<00:16,  1.95it/s]\u001b[A\n",
            "Training:  94%|█████████▎| 453/484 [04:35<00:17,  1.78it/s]\u001b[A\n",
            "Training:  94%|█████████▍| 454/484 [04:36<00:16,  1.79it/s]\u001b[A\n",
            "Training:  94%|█████████▍| 455/484 [04:36<00:15,  1.85it/s]\u001b[A\n",
            "Training:  94%|█████████▍| 456/484 [04:37<00:15,  1.76it/s]\u001b[A\n",
            "Training:  94%|█████████▍| 457/484 [04:37<00:14,  1.92it/s]\u001b[A\n",
            "Training:  95%|█████████▍| 458/484 [04:38<00:14,  1.79it/s]\u001b[A\n",
            "Training:  95%|█████████▍| 459/484 [04:38<00:13,  1.80it/s]\u001b[A\n",
            "Training:  95%|█████████▌| 460/484 [04:39<00:13,  1.74it/s]\u001b[A\n",
            "Training:  95%|█████████▌| 461/484 [04:40<00:13,  1.66it/s]\u001b[A\n",
            "Training:  95%|█████████▌| 462/484 [04:40<00:12,  1.74it/s]\u001b[A\n",
            "Training:  96%|█████████▌| 463/484 [04:41<00:11,  1.76it/s]\u001b[A\n",
            "Training:  96%|█████████▌| 464/484 [04:41<00:10,  1.83it/s]\u001b[A\n",
            "Training:  96%|█████████▌| 465/484 [04:42<00:10,  1.77it/s]\u001b[A\n",
            "Training:  96%|█████████▋| 466/484 [04:42<00:09,  1.97it/s]\u001b[A\n",
            "Training:  96%|█████████▋| 467/484 [04:43<00:08,  1.93it/s]\u001b[A\n",
            "Training:  97%|█████████▋| 468/484 [04:43<00:08,  1.87it/s]\u001b[A\n",
            "Training:  97%|█████████▋| 469/484 [04:44<00:08,  1.81it/s]\u001b[A\n",
            "Training:  97%|█████████▋| 470/484 [04:45<00:07,  1.77it/s]\u001b[A\n",
            "Training:  97%|█████████▋| 471/484 [04:45<00:06,  1.97it/s]\u001b[A\n",
            "Training:  98%|█████████▊| 472/484 [04:46<00:06,  1.88it/s]\u001b[A\n",
            "Training:  98%|█████████▊| 473/484 [04:46<00:06,  1.79it/s]\u001b[A\n",
            "Training:  98%|█████████▊| 474/484 [04:47<00:05,  1.88it/s]\u001b[A\n",
            "Training:  98%|█████████▊| 475/484 [04:47<00:04,  1.99it/s]\u001b[A\n",
            "Training:  98%|█████████▊| 476/484 [04:47<00:03,  2.13it/s]\u001b[A\n",
            "Training:  99%|█████████▊| 477/484 [04:48<00:03,  2.24it/s]\u001b[A\n",
            "Training:  99%|█████████▉| 478/484 [04:48<00:02,  2.29it/s]\u001b[A\n",
            "Training:  99%|█████████▉| 479/484 [04:49<00:02,  2.06it/s]\u001b[A\n",
            "Training:  99%|█████████▉| 480/484 [04:49<00:02,  1.89it/s]\u001b[A\n",
            "Training:  99%|█████████▉| 481/484 [04:50<00:01,  1.85it/s]\u001b[A\n",
            "Training: 100%|█████████▉| 482/484 [04:51<00:01,  1.88it/s]\u001b[A\n",
            "Training: 100%|█████████▉| 483/484 [04:51<00:00,  2.08it/s]\u001b[A\n",
            "Training: 100%|██████████| 484/484 [04:51<00:00,  1.66it/s]\n",
            "100%|██████████| 1/1 [04:51<00:00, 291.63s/it]\n"
          ]
        }
      ],
      "source": [
        "best_model = train(model, config, train_dataloader, val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dp7Ezy8YE3ms"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}