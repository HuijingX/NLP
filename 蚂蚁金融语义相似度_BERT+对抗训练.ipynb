{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "蚂蚁金融语义相似度_BERT+对抗训练.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeqZBz6Rf_Ub"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "config = {\n",
        "    'train_file_path': '/content/drive/MyDrive/train.json',\n",
        "    'dev_file_path': '/content/drive/MyDrive/dev.json',\n",
        "    'test_file_path': '/content/drive/MyDrive/test.json',\n",
        "    'output_path': '/content/drive/MyDrive/output',\n",
        "    'model_path': '/content/drive/MyDrive/BERT_model',\n",
        "    'batch_size': 64,\n",
        "    'num_epoches': 1,\n",
        "    'max_seq_len': 64,\n",
        "    'learning_rate': 2e-5,\n",
        "    'eps': 0.1,\n",
        "    'alpha': 0.3,\n",
        "    'adv': 'fgm',\n",
        "    'warmup_ratio': 0.05,\n",
        "    'weight_decay': 0.01,\n",
        "    'use_bucket': True,\n",
        "    'bucket_multiplier': 200,\n",
        "    'device': 'cuda',\n",
        "    'n_gpus': 0,\n",
        "    'use_amp': True,\n",
        "    'logging_step': 300,\n",
        "    'ema_start_step': 500,\n",
        "    'ema_start': False,\n",
        "    'seed': 2021\n",
        "}\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "  config['device'] = 'cpu' \n",
        "else:\n",
        "  config['n_gpus'] = torch.cuda.device_count()\n",
        "  config['batch_size'] *= config['n_gpus']\n",
        "\n",
        "if not os.path.exists(config['output_path']):\n",
        "  os.makedirs((config['output_path']))\n",
        "\n",
        "def seed_everything(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(config['seed'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXQZFz9FhDtQ"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def parse_data(path, data_type='train'):\n",
        "  sentence_a = []\n",
        "  sentence_b = []\n",
        "  labels = []\n",
        "  with open(path, 'r', encoding='utf8') as f:\n",
        "    for line in tqdm(f.readlines(), desc=f'Reading {data_type} data'):\n",
        "      line = json.loads(line)\n",
        "      sentence_a.append(line['sentence1'])\n",
        "      sentence_b.append(line['sentence2'])\n",
        "      if data_type!='test':\n",
        "        labels.append(int(line['label']))\n",
        "      else:\n",
        "        labels.append(0)\n",
        "\n",
        "  df = pd.DataFrame(zip(sentence_a,sentence_b,labels), columns=['text_a', 'text_b', 'labels'])\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgBRXHMjle8l"
      },
      "outputs": [],
      "source": [
        "def build_bert_inputs(inputs, label, sentence_a, sentence_b, tokenizer):\n",
        "  inputs_dict = tokenizer.encode_plus(sentence_a, sentence_b, add_special_tokens=True, return_token_type_ids=True, return_attention_mask=True)\n",
        "  inputs['input_ids'].append(inputs_dict['input_ids'])\n",
        "  inputs['token_type_ids'].append(inputs_dict['token_type_ids'])\n",
        "  inputs['attention_mask'].append(inputs_dict['attention_mask'])\n",
        "  inputs['labels'].append(label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWFaBu84le_B"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "def read_data(config, tokenizer):\n",
        "  train_df = parse_data(config['train_file_path'], data_type='train')\n",
        "  dev_df = parse_data(config['dev_file_path'], data_type='dev')\n",
        "  test_df = parse_data(config['test_file_path'], data_type='test')\n",
        "\n",
        "  data_df = {'train': train_df, 'dev': dev_df, 'test': test_df}\n",
        "\n",
        "  processed_data = {}\n",
        "\n",
        "  for data_type, df in data_df.items():\n",
        "    inputs = defaultdict(list)\n",
        "    for i, row in tqdm(df.iterrows(), desc=f'Preprocessing {data_type} data', total=len(df)):\n",
        "      label = row[2]\n",
        "      sentence_a, sentence_b = row[0], row[1]\n",
        "      build_bert_inputs(inputs, label, sentence_a, sentence_b, tokenizer)\n",
        "\n",
        "    processed_data[data_type] = inputs\n",
        "  \n",
        "  return processed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzKYl4h4f_Uc",
        "outputId": "636443e7-11fe-487e-c9d7-077caf85eaf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading train data: 100%|██████████| 34334/34334 [00:00<00:00, 225174.91it/s]\n",
            "Reading dev data: 100%|██████████| 4316/4316 [00:00<00:00, 192829.24it/s]\n",
            "Reading test data: 100%|██████████| 3861/3861 [00:00<00:00, 226695.33it/s]\n",
            "Preprocessing train data: 100%|██████████| 34334/34334 [00:24<00:00, 1424.02it/s]\n",
            "Preprocessing dev data: 100%|██████████| 4316/4316 [00:03<00:00, 1432.20it/s]\n",
            "Preprocessing test data: 100%|██████████| 3861/3861 [00:02<00:00, 1415.03it/s]\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(config['model_path'])\n",
        "\n",
        "dt = read_data(config, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZYpV86spaAh",
        "outputId": "6cbcf7aa-06f4-476b-8a3f-0031109efd28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 6010, 6009, 955, 1446, 5023, 7583, 6820, 3621, 1377, 809, 2940, 2768, 1044, 2622, 1400, 3315, 1408, 102, 955, 1446, 3300, 1044, 2622, 1168, 3309, 6820, 3315, 1408, 102]\n"
          ]
        }
      ],
      "source": [
        "print(dt['train']['input_ids'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCEJt2Npf_Uc"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "class AFQMCDataset(Dataset):\n",
        "  def __init__(self, data_dict):\n",
        "    super(AFQMCDataset, self).__init__()\n",
        "    self.data_dict = data_dict\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # ---------------------------------------------#\n",
        "    data = (self.data_dict['input_ids'][idx],\n",
        "            self.data_dict['token_type_ids'][idx],\n",
        "            self.data_dict['attention_mask'][idx],\n",
        "            self.data_dict['labels'][idx])\n",
        "    return data\n",
        "    # ---------------------------------------------#\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data_dict['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCA_Jz-Qf_Ud"
      },
      "outputs": [],
      "source": [
        "class Collator():\n",
        "  def __init__(self, max_seq_len, tokenizer):\n",
        "    self.max_seq_len = max_seq_len\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def pad_and_truncate(self, input_ids_list, token_type_ids_list, attention_mask_list, labels_list, max_seq_len):\n",
        "    input_ids = torch.zeros((len(input_ids_list), max_seq_len), dtype=torch.long)\n",
        "    token_type_ids = torch.zeros_like(input_ids)\n",
        "    attention_mask = torch.zeros_like(input_ids)\n",
        "\n",
        "    for i in range(len(input_ids_list)):\n",
        "      seq_len = len(input_ids_list[i])\n",
        "\n",
        "      if seq_len <= max_seq_len:\n",
        "        input_ids[i,:seq_len] = torch.tensor(input_ids_list[i], dtype=torch.long)\n",
        "        token_type_ids[i,:seq_len] = torch.tensor(token_type_ids_list[i], dtype=torch.long)\n",
        "        attention_mask[i,:seq_len] = torch.tensor(attention_mask_list[i], dtype=torch.long)\n",
        "      else:\n",
        "        input_ids[i] = torch.tensor(input_ids_list[i][:max_seq_len-1] + [self.tokenizer.sep_token_id], dtype=torch.long)\n",
        "        token_type_ids[i] = torch.tensor(token_type_ids_list[i][:max_seq_len], dtype=torch.long)\n",
        "        attention_mask[i] = torch.tensor(attention_mask_list[i][:max_seq_len], dtype=torch.long)\n",
        "\n",
        "    labels = torch.tensor(labels_list, dtype=torch.long)\n",
        "    return input_ids, token_type_ids, attention_mask, labels\n",
        "\n",
        "  def __call__(self, examples):\n",
        "    input_ids_list, token_type_ids_list, attention_mask_list, labels_list = list(zip(*examples))\n",
        "    cur_max_seq_len = max(len(input_id) for input_id in input_ids_list)\n",
        "    max_seq_len = min(cur_max_seq_len, self.max_seq_len)\n",
        "\n",
        "    input_ids, token_type_ids, attention_mask, labels = self.pad_and_truncate(input_ids_list, \n",
        "                                                                              token_type_ids_list, attention_mask_list, \n",
        "                                                                              labels_list, max_seq_len)\n",
        "    \n",
        "    data_dict = {\n",
        "        'input_ids': input_ids,\n",
        "        'token_type_ids': token_type_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'labels': labels\n",
        "    }\n",
        "\n",
        "    return data_dict\n",
        "                        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RX_TCv9CvM47"
      },
      "outputs": [],
      "source": [
        "collate_fn = Collator(config['max_seq_len'], tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVFKCpfF7u2P"
      },
      "outputs": [],
      "source": [
        "from bucket_sampler import BucketBatchSampler\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import RandomSampler\n",
        "\n",
        "def build_dataloader(config, data, collate_fn):\n",
        "  train_dataset = AFQMCDataset(data['train'])\n",
        "  dev_dataset = AFQMCDataset(data['dev'])\n",
        "  test_dataset = AFQMCDataset(data['test'])\n",
        "\n",
        "  if config['use_bucket']:\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    bucket_sampler = BucketBatchSampler(train_sampler, batch_size=config['batch_size'],\n",
        "                                        drop_last=False, sort_key=lambda x:len(train_dataset[x][0]),\n",
        "                                        bucket_size_multiplier=config['bucket_multiplier']\n",
        "                                       )\n",
        "    train_dataloader = DataLoader(dataset=train_dataset, batch_sampler=bucket_sampler,\n",
        "                                  num_workers=4, collate_fn=collate_fn)\n",
        "  else:\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'],\n",
        "                                  shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
        " \n",
        "  dev_dataloader = DataLoader(dev_dataset, batch_size=config['batch_size'],\n",
        "                                  shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=config['batch_size'],\n",
        "                                  shuffle=False, num_workers=4, collate_fn=collate_fn) \n",
        "  return train_dataloader, dev_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c133vqsM-ncm",
        "outputId": "3244b807-039c-419a-df70-ca16cb75432d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "train_dataloader, dev_dataloader, test_dataloader = build_dataloader(config, dt, collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sThtqyOv_VNg",
        "outputId": "62f058c7-1078-4a5e-89ea-18f1a6def8d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[ 101, 2376, 2769,  ..., 6820, 7178,  102],\n",
            "        [ 101, 5709, 1446,  ..., 3309, 1408,  102],\n",
            "        [ 101, 4385, 1762,  ...,  749, 1435,  102],\n",
            "        ...,\n",
            "        [ 101, 6010, 6009,  ..., 4500, 1905,  102],\n",
            "        [ 101,  955, 1446,  ..., 3621, 1408,  102],\n",
            "        [ 101,  711,  784,  ...,  115,  115,  102]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
            "        [0, 0, 0,  ..., 1, 1, 1],\n",
            "        [0, 0, 0,  ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 1, 1, 1],\n",
            "        [0, 0, 0,  ..., 1, 1, 1],\n",
            "        [0, 0, 0,  ..., 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
            "        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])}\n"
          ]
        }
      ],
      "source": [
        "for i in train_dataloader:\n",
        "  print(i)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nda-hfnUf_Ud"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score,accuracy_score\n",
        "def evaluation(model, config, val_dataloader):\n",
        "  model.eval()\n",
        "  preds = []\n",
        "  labels = []\n",
        "  val_loss = 0.\n",
        "  val_iterator = tqdm(val_dataloader, desc='Evaluation', total=len(val_dataloader))\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in val_iterator:  \n",
        "      # ---------------------------------------------#\n",
        "      labels.append(batch['labels'])\n",
        "      batch_cuda = {item: value.to(config['device']) for item, value in list(batch.items())}\n",
        "\n",
        "      loss, logits = model(**batch_cuda)[:2]\n",
        "      if config['n_gpus'] > 1:\n",
        "        loss = loss.mean()\n",
        "      # ---------------------------------------------#\n",
        "      val_loss += loss.item()\n",
        "      \n",
        "      preds.append(logits.argmax(dim=-1).detach().cpu())\n",
        "\n",
        "  avg_val_loss = val_loss/len(val_dataloader)\n",
        "  labels = torch.cat(labels, dim=0).numpy()\n",
        "  preds = torch.cat(preds, dim=0).numpy()\n",
        "\n",
        "  f1 = f1_score(labels, preds, average='macro')\n",
        "  acc = accuracy_score(labels, preds)\n",
        "\n",
        "  return avg_val_loss, f1, acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-a9x-jMFFa0"
      },
      "outputs": [],
      "source": [
        "from types import new_class\n",
        "class EMA:\n",
        "  def __init__(self, model, decay):\n",
        "    self.model = model\n",
        "    self.decay = decay\n",
        "    self.shadow = {}\n",
        "    self.backup = {}\n",
        "    self.register()\n",
        "\n",
        "  def register(self):\n",
        "    for name, param in self.model.named_parameters():\n",
        "      if param.requires_grad:\n",
        "        self.shadow[name] = param.data.clone()\n",
        "\n",
        "  def update(self):\n",
        "    for name, param in self.model.named_parameters():\n",
        "      if param.requires_grad:\n",
        "        assert name in self.shadow\n",
        "        new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
        "        self.shadow[name] = new_average.clone()\n",
        "\n",
        "  def apply_shadow(self):\n",
        "    for name, param in self.model.named_parameters():\n",
        "      if param.requires_grad:\n",
        "        assert name in self.shadow\n",
        "        self.backup[name] = param.data\n",
        "        param.data = self.shadow[name]\n",
        "\n",
        "  def resrore(self):\n",
        "    for name, param in self.model.named_parameters():\n",
        "      if param.requires_grad:\n",
        "        assert name in self.backup\n",
        "        param.data = self.backup[name]\n",
        "    self.backup = {}\n",
        "                                            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVaqEi44FQIl"
      },
      "outputs": [],
      "source": [
        "from extra_loss import *\n",
        "from extra_optim import *\n",
        "from extra_fgm import *\n",
        "from extra_pgd import *\n",
        "from transformers import AdamW, BertForSequenceClassification\n",
        "from torch.cuda import amp\n",
        "from tqdm import trange\n",
        "def train(config, train_dataloader, dev_dataloader):\n",
        "\n",
        "  model = BertForSequenceClassification.from_pretrained(config['model_path'])\n",
        "\n",
        "  param_optimizer = list(model.named_parameters())\n",
        "\n",
        "  # 实例化 GradScaler 对象\n",
        "  scaler = amp.GradScaler(enabled=config['use_amp'])\n",
        "\n",
        "  no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "  optimizer_grouped_parameters = [\n",
        "    {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     \"weight_decay\": config['weight_decay']},\n",
        "    {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     \"weight_decay\": 0.0}\n",
        "  ]\n",
        "\n",
        "  optimizer = AdamW(optimizer_grouped_parameters, lr=config['learning_rate'], eps=1e-8)\n",
        "  optimizer = Lookahead(optimizer, 5, 1)\n",
        "  total_steps = config['num_epoches'] * len(train_dataloader)\n",
        "\n",
        "  lr_scheduler = WarmupLinearSchedule(optimizer,\n",
        "                                      warmup_steps=int(config['warmup_ratio']*total_steps),\n",
        "                                      t_total=total_steps)\n",
        "\n",
        "  model.to(config['device'])\n",
        "\n",
        "  if config['adv'] == 'fgm':\n",
        "    fgm = FGM(model)\n",
        "  else:\n",
        "    pgd = PGD(model)\n",
        "    K = 3\n",
        "\n",
        "  epoches_iterator = trange(config['num_epoches'])\n",
        "\n",
        "  global_steps = 0\n",
        "  train_loss = 0.\n",
        "  logging_loss = 0.\n",
        "  best_acc = 0.\n",
        "  best_model_path = ''\n",
        "\n",
        "  if config['n_gpus'] > 1:\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "  for epoch in epoches_iterator:\n",
        "    train_iterator = tqdm(train_dataloader, desc='Training', total=len(train_dataloader))\n",
        "    model.train()\n",
        "    \n",
        "    for batch in train_iterator:\n",
        "      batch_cuda = {item: value.to(config['device']) for item, value in batch.items()}\n",
        "\n",
        "      with amp.autocast(enabled=config['use_amp']):\n",
        "        loss = model(**batch_cuda)[0]\n",
        "        if config['n_gpus'] > 1:\n",
        "          loss = loss.mean()\n",
        "\n",
        "      scaler.scale(loss).backward()\n",
        "\n",
        "      if config['adv'] == 'fgm':\n",
        "        fgm.attack(epsilon=config['eps'])\n",
        "          \n",
        "        with amp.autocast(enabled=config['use_amp']):\n",
        "          loss_adv = model(**batch_cuda)[0]\n",
        "          if config['n_gpus'] > 1:\n",
        "            loss_adv = loss_adv.mean()\n",
        "\n",
        "        scaler.scale(loss_adv).backward()\n",
        "        fgm.restore()\n",
        "      else:\n",
        "        pgd.backup_grad()\n",
        "        for t in range(K):\n",
        "          pgd.attack(epsilon=config['eps'], alpha=config['alpha'], is_first_attack=(t==0))\n",
        "          if t != K-1:\n",
        "            model.zero_grad()\n",
        "          else:\n",
        "            pgd.restore_grad()\n",
        "\n",
        "          with amp.autocast(enabled=config['use_amp']):\n",
        "            loss_adv = model(**batch_cuda)[0]\n",
        "            if config['n_gpus'] > 1:\n",
        "              loss_adv = loss_adv.mean()\n",
        "\n",
        "          scaler.scale(loss_adv).backward()\n",
        "        pgd.restore()\n",
        "\n",
        "      scaler.step(optimizer)\n",
        "      scaler.update()\n",
        "\n",
        "      lr_scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      if config['ema_start']:\n",
        "        ema.update()\n",
        "\n",
        "      train_loss += loss.item()\n",
        "      global_steps +=1\n",
        "\n",
        "      train_iterator.set_postfix_str(f'running training loss: {loss.item():.4f}')\n",
        "\n",
        "\n",
        "      if global_steps % config['logging_step'] == 0:\n",
        "        if global_steps >= config['ema_start_step'] and not config['ema_start']:\n",
        "          print('\\n>>>EMA starting...')\n",
        "          config['ema_start'] = True\n",
        "          ema = EMA(model.module if hasattr(model, 'module') else model, decay=0.999)\n",
        "\n",
        "        print_train_loss = (train_loss - logging_loss)/ config['logging_step'] \n",
        "        logging_loss = train_loss\n",
        "\n",
        "        if config['ema_start']:\n",
        "          ema.apply_shadow()\n",
        "\n",
        "        val_loss, f1, acc = evaluation(model, config, dev_dataloader)\n",
        "        print_log = f'\\n>>> training loss: {print_train_loss:.6f}, valid loss: {val_loss:.6f}, '\n",
        "\n",
        "        if acc > best_acc:\n",
        "          model_save_path = os.path.join(config['output_path'],\n",
        "                                         f'checkpoint-{global_steps}-{acc:.6f}')\n",
        "          model_to_save = model.module if hasattr(model, 'module') else model\n",
        "          model_to_save.save_pretrained(model_save_path)\n",
        "          best_acc = acc\n",
        "          best_model_path = model_save_path\n",
        "        \n",
        "        print_log += f'valid f1: {f1:.6f}, valid acc: {acc:.6f}'\n",
        "        print(print_log)\n",
        "        model.train()\n",
        "\n",
        "        if config['ema_start']:\n",
        "          ema.restore()\n",
        "\n",
        "  return model, best_model_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIg8rgwzrRTe"
      },
      "outputs": [],
      "source": [
        "best_model, best_model_path = train(config, train_dataloader, dev_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMgfXXm3f_Ue"
      },
      "outputs": [],
      "source": [
        "def predict(config, id2label, model, test_dataloader):\n",
        "  model.eval()\n",
        "  test_iterator = tqdm(test_dataloader, desc='Predicting', total=len(test_dataloader))\n",
        "  test_preds =[]\n",
        "\n",
        "  with torch.no_grad():\n",
        "     for batch in test_iterator:\n",
        "       # ---------------------------------------------#\n",
        "       batch = {k: v.to(config['device']) for k,v in batch.items}\n",
        "       logits = model(**batch)[1]\n",
        "       # ---------------------------------------------#\n",
        "       test_preds.append(logits.argmax(dim=-1).detach().cpu())\n",
        "  test_preds = torch.cat(test_preds, dim=0).numpy()\n",
        "  test_preds = [id2label[idx] for idx in test_preds]\n",
        "\n",
        "  test_df = pd.read_csv(config['test_file_path'], sep=',')\n",
        "  test_df['preds'] = test_preds\n",
        "  \n",
        "  return test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LIGyiFDf_Ug"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}