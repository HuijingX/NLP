{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "蚂蚁金融语义相似度_BERT+UDA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpsa5eyceZDK"
      },
      "outputs": [],
      "source": [
        "from bucket_sampler import SortedSampler, BucketBatchSampler\n",
        "from EMA import *\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "config = {\n",
        "    'train_file_path': '/content/drive/MyDrive/train.json',\n",
        "    'dev_file_path': '/content/drive/MyDrive/dev.json',\n",
        "    'test_file_path': '/content/drive/MyDrive/test.json',\n",
        "    'output_path': '/content/drive/MyDrive/output',\n",
        "    'model_path': '/content/drive/MyDrive/BERT_model',\n",
        "    'batch_size': 16,\n",
        "    'num_epoches': 1,\n",
        "    'max_seq_len': 64,\n",
        "    'learning_rate': 2e-5,\n",
        "    'weight_decay': 0.01,\n",
        "    'use_bucket': True,\n",
        "    'bucket_multiplier': 200,\n",
        "    'unsup_data_ratio': 1.5,\n",
        "    'uda_softmax_temp': 0.4,\n",
        "    'uda_confidence_threshold': 0.8,\n",
        "    'device': 'cuda',\n",
        "    'n_gpus': 0,\n",
        "    'logging_step': 300,\n",
        "    'ema_start_step': 500,\n",
        "    'ema_start': False,\n",
        "    'seed': 2021\n",
        "}\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "  config['device'] = 'cpu' \n",
        "else:\n",
        "  config['n_gpus'] = torch.cuda.device_count()\n",
        "  config['batch_size'] *= config['n_gpus']\n",
        "\n",
        "def seed_everything(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(config['seed'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNjaA811hmST"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def parse_data(path, data_type='train'):\n",
        "  sentence_a = []\n",
        "  sentence_b = []\n",
        "  labels = []\n",
        "  with open(path, 'r', encoding='utf8') as f:\n",
        "    for line in tqdm(f.readlines(), desc=f'Reading {data_type} data'):\n",
        "      line = json.loads(line)\n",
        "      sentence_a.append(line['sentence1'])\n",
        "      sentence_b.append(line['sentence2'])\n",
        "      if data_type!='test':\n",
        "        labels.append(int(line['label']))\n",
        "      else:\n",
        "        labels.append(0)\n",
        "\n",
        "  df = pd.DataFrame(zip(sentence_a,sentence_b,labels), columns=['text_a', 'text_b', 'labels'])\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpU27lZ_feK8"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(config['model_path'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Su6hO9EfeNQ"
      },
      "outputs": [],
      "source": [
        "def build_bert_inputs(inputs, label, sentence_a, sentence_b, tokenizer):\n",
        "  inputs_dict = tokenizer.encode_plus(sentence_a, sentence_b, add_special_tokens=True,\n",
        "                                      return_token_type_ids=True, return_attention_mask=True)\n",
        "  inputs['input_ids'].append(inputs_dict['input_ids'])\n",
        "  inputs['token_type_ids'].append(inputs_dict['token_type_ids'])\n",
        "  inputs['attention_mask'].append(inputs_dict['attention_mask'])\n",
        "  inputs['labels'].append(label)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Og7BDuzAfePj"
      },
      "outputs": [],
      "source": [
        "def build_unsup_bert_inputs(inputs, label, sentence_a, sentence_b, tokenizer):\n",
        "  lr_inputs_dict = tokenizer.encode_plus(sentence_a, sentence_b, add_special_tokens=True,\n",
        "                                         return_token_type_ids=True, return_attention_mask=True)\n",
        "  rl_inputs_dict = tokenizer.encode_plus(sentence_b, sentence_a, add_special_tokens=True,\n",
        "                                         return_token_type_ids=True, return_attention_mask=True)\n",
        "  \n",
        "  inputs['input_ids'].append((lr_inputs_dict['input_ids'],rl_inputs_dict['input_ids']))\n",
        "  inputs['token_type_ids'].append((lr_inputs_dict['token_type_ids'],rl_inputs_dict['token_type_ids']))\n",
        "  inputs['attention_mask'].append((lr_inputs_dict['attention_mask'],rl_inputs_dict['attention_mask']))\n",
        "  inputs['labels'].append(label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E04WxC1JfeT0"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "def read_data(config, tokenizer):\n",
        "  train_df = parse_data(config['train_file_path'], data_type='train')\n",
        "  dev_df = parse_data(config['dev_file_path'], data_type='dev')\n",
        "  test_df = parse_data(config['test_file_path'], data_type='test')\n",
        "\n",
        "  data_df = {'train': train_df, 'dev': dev_df, 'test': test_df}\n",
        "  processed_data = {}\n",
        "  unsup_data = defaultdict(list)\n",
        "\n",
        "  for data_type, df in data_df.items():\n",
        "    inputs = defaultdict(list)\n",
        "\n",
        "    for i, row in tqdm(df.iterrows(), desc=f'Preprocessing {data_type} data', total=len(df)):\n",
        "      label = 0 if data_type == 'test' else row[2]\n",
        "      sentence_a, sentence_b = row[0], row[1]\n",
        "      build_bert_inputs(inputs, label, sentence_a, sentence_b, tokenizer)\n",
        "\n",
        "      if data_type.startswith('test'):\n",
        "        build_bert_inputs(inputs, label, sentence_b, sentence_a, tokenizer)\n",
        "\n",
        "      build_unsup_bert_inputs(unsup_data, label, sentence_a, sentence_b, tokenizer)\n",
        "\n",
        "    processed_data[data_type] = inputs\n",
        "\n",
        "  processed_data['unsup_data'] = unsup_data\n",
        "  return processed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsSnzJbwfeV_",
        "outputId": "fd845b73-277a-48de-d436-0b0f0e77b8b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading train data: 100%|██████████| 34334/34334 [00:00<00:00, 214124.73it/s]\n",
            "Reading dev data: 100%|██████████| 4316/4316 [00:00<00:00, 197353.19it/s]\n",
            "Reading test data: 100%|██████████| 3861/3861 [00:00<00:00, 198992.49it/s]\n",
            "Preprocessing train data: 100%|██████████| 34334/34334 [01:23<00:00, 413.23it/s]\n",
            "Preprocessing dev data: 100%|██████████| 4316/4316 [00:08<00:00, 514.24it/s]\n",
            "Preprocessing test data: 100%|██████████| 3861/3861 [00:09<00:00, 409.51it/s]\n"
          ]
        }
      ],
      "source": [
        "data = read_data(config, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in data['train']['input_ids']:\n",
        "  print(i)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRsZrJbKf-3u",
        "outputId": "cb88400c-bc94-4799-b179-51d639fa1849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 6010, 6009, 955, 1446, 5023, 7583, 6820, 3621, 1377, 809, 2940, 2768, 1044, 2622, 1400, 3315, 1408, 102, 955, 1446, 3300, 1044, 2622, 1168, 3309, 6820, 3315, 1408, 102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OF2JKcImfeXx"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "class AFQMCDataset(Dataset):\n",
        "  def __init__(self, data_dict):\n",
        "    super(AFQMCDataset, self).__init__()\n",
        "    self.data_dict = data_dict\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    data = (self.data_dict['input_ids'][index], self.data_dict['token_type_ids'][index],\n",
        "            self.data_dict['attention_mask'][index], self.data_dict['labels'][index])\n",
        "    return data\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.data_dict['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNS14W6Wkug5"
      },
      "outputs": [],
      "source": [
        "class Collator():\n",
        "  def __init__(self, max_seq_len, tokenizer):\n",
        "    self.max_seq_len = max_seq_len\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def pad_and_truncate(self, input_ids_list, token_type_ids_list, attention_mask_list, labels_list, max_seq_len):\n",
        "    input_ids = torch.zeros((len(input_ids_list), max_seq_len), dtype=torch.long)\n",
        "    token_type_ids = torch.zeros_like(input_ids)\n",
        "    attention_mask = torch.zeros_like(input_ids)\n",
        "\n",
        "    for i in range(len(input_ids_list)):\n",
        "      seq_len = len(input_ids_list[i])\n",
        "\n",
        "      if seq_len <= max_seq_len:\n",
        "        input_ids[i,:seq_len] = torch.tensor(input_ids_list[i], dtype=torch.long)\n",
        "        token_type_ids[i,:seq_len] = torch.tensor(token_type_ids_list[i], dtype=torch.long)\n",
        "        attention_mask[i,:seq_len] = torch.tensor(attention_mask_list[i], dtype=torch.long)\n",
        "      else:\n",
        "        input_ids[i] = torch.tensor(input_ids_list[i][:max_seq_len-1] + [self.tokenizer.sep_token_id], dtype=torch.long)\n",
        "        token_type_ids[i] = torch.tensor(token_type_ids_list[i][:max_seq_len])\n",
        "        attention_mask[i] = torch.tensor(attention_mask_list[i][:max_seq_len])\n",
        "\n",
        "    labels = torch.tensor(labels_list, dtype=torch.long)\n",
        "    return input_ids, token_type_ids, attention_mask, labels\n",
        "\n",
        "\n",
        "  def __call__(self, examples):\n",
        "    input_ids_list, token_type_ids_list, attention_mask_list, labels_list = list(zip(*examples))\n",
        "    cur_max_seq_len = max(len(input_ids) for input_ids in input_ids_list)\n",
        "    max_seq_len = min(cur_max_seq_len, self.max_seq_len)\n",
        "\n",
        "    input_ids, token_type_ids, attention_mask, labels = self.pad_and_truncate(input_ids_list, token_type_ids_list, attention_mask_list, labels_list, max_seq_len)\n",
        "\n",
        "    data_dict = {\n",
        "        'input_ids': input_ids,\n",
        "        'token_type_ids': token_type_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'labels': labels\n",
        "    }\n",
        "    return data_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xRTI_88nzw3"
      },
      "outputs": [],
      "source": [
        "collate_fn = Collator(config['max_seq_len'], tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLbL_scDn6rG"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "class UnsupAFQMCDataset(Dataset):\n",
        "  def __init__(self, data_dict):\n",
        "    super(UnsupAFQMCDataset, self).__init__()\n",
        "    self.data_dict = data_dict\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    input_ids = self.data_dict['input_ids'][index]\n",
        "    token_type_ids = self.data_dict['token_type_ids'][index]\n",
        "    attention_mask = self.data_dict['attention_mask'][index]\n",
        "    labels = self.data_dict['labels'][index]\n",
        "    return (input_ids[0], token_type_ids[0], attention_mask[0],\n",
        "            input_ids[1], token_type_ids[1], attention_mask[1], labels)\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.data_dict['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWv7eDngodfy"
      },
      "outputs": [],
      "source": [
        "class UnsupCollator():\n",
        "  def __init__(self, max_seq_len, tokenizer):\n",
        "    self.max_seq_len = max_seq_len\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def pad_and_truncate(self, input_ids_list, token_type_ids_list, attention_mask_list, labels_list, max_seq_len):\n",
        "    input_ids = torch.zeros((len(input_ids_list), max_seq_len), dtype=torch.long)\n",
        "    token_type_ids = torch.zeros_like(input_ids)\n",
        "    attention_mask = torch.zeros_like(input_ids)\n",
        "\n",
        "    for i in range(len(input_ids_list)):\n",
        "      seq_len = len(input_ids_list[i])\n",
        "\n",
        "      if seq_len <= max_seq_len:\n",
        "        input_ids[i,:seq_len] = torch.tensor(input_ids_list[i], dtype=torch.long)\n",
        "        token_type_ids[i,:seq_len] = torch.tensor(token_type_ids_list[i], dtype=torch.long)\n",
        "        attention_mask[i,:seq_len] = torch.tensor(attention_mask_list[i], dtype=torch.long)\n",
        "      else:\n",
        "        input_ids[i] = torch.tensor(input_ids_list[i][:max_seq_len-1] + [self.tokenizer.sep_token_id], dtype=torch.long)\n",
        "        token_type_ids[i] = torch.tensor(token_type_ids_list[i][:max_seq_len])\n",
        "        attention_mask[i] = torch.tensor(attention_mask_list[i][:max_seq_len])\n",
        "\n",
        "    labels = torch.tensor(labels_list, dtype=torch.long)\n",
        "    return input_ids, token_type_ids, attention_mask, labels\n",
        "\n",
        "\n",
        "  def __call__(self, examples):\n",
        "    (ab_input_ids_list, ab_token_type_ids_list, ab_attention_mask_list, \n",
        "     ba_input_ids_list, ba_token_type_ids_list, ba_attention_mask_list, labels_list) = list(zip(*examples))\n",
        "    cur_max_seq_len = max(len(input_ids) for input_ids in ab_input_ids_list)\n",
        "    max_seq_len = min(cur_max_seq_len, self.max_seq_len)\n",
        "\n",
        "    ab_input_ids, ab_token_type_ids, ab_attention_mask, labels = self.pad_and_truncate(ab_input_ids_list, ab_token_type_ids_list, ab_attention_mask_list, labels_list, max_seq_len)\n",
        "    ba_input_ids, ba_token_type_ids, ba_attention_mask, labels = self.pad_and_truncate(ba_input_ids_list, ba_token_type_ids_list, ba_attention_mask_list, labels_list, max_seq_len)\n",
        "\n",
        "    data_dict = {\n",
        "        'ab_input_ids': ab_input_ids,\n",
        "        'ab_token_type_ids': ab_token_type_ids,\n",
        "        'ab_attention_mask': ab_attention_mask,\n",
        "        'ba_input_ids': ba_input_ids,\n",
        "        'ba_token_type_ids': ba_token_type_ids,\n",
        "        'ba_attention_mask': ba_attention_mask,\n",
        "        'labels': labels\n",
        "    }\n",
        "    return data_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76sfRU2ApVKA"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler\n",
        "def build_dataloader(config, data, tokenizer):\n",
        "  train_dataset = AFQMCDataset(data['train'])\n",
        "  dev_dataset = AFQMCDataset(data['dev'])\n",
        "  test_dataset = AFQMCDataset(data['test'])\n",
        "\n",
        "  unsup_dataset = UnsupAFQMCDataset(data['unsup_data'])\n",
        "\n",
        "  collate_fn = Collator(config['max_seq_len'], tokenizer)\n",
        "  unsup_collate_fn = UnsupCollator(config['max_seq_len'], tokenizer)\n",
        "\n",
        "  if config['use_bucket']:\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    bucket_sampler = BucketBatchSampler(train_sampler, batch_size=config['batch_size'],\n",
        "                                        drop_last=False, sort_key=lambda x:len(train_dataset[x][0]),\n",
        "                                        bucket_size_multiplier=config['bucket_multiplier'])\n",
        "    train_dataloader = DataLoader(dataset=train_dataset, batch_sampler=bucket_sampler, num_workers=4, collate_fn=collate_fn)\n",
        "\n",
        "    unsup_sampler = RandomSampler(unsup_dataset)\n",
        "    unsup_bucket_sampler = BucketBatchSampler(unsup_sampler, batch_size=int(config['batch_size']*config['unsup_data_ratio']),\n",
        "                                        drop_last=False, sort_key=lambda x:len(unsup_dataset[x][0]),\n",
        "                                        bucket_size_multiplier=config['bucket_multiplier'])\n",
        "    unsup_dataloader = DataLoader(dataset=unsup_dataset, batch_sampler=unsup_bucket_sampler, num_workers=4, collate_fn=unsup_collate_fn)\n",
        "\n",
        "  else:\n",
        "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
        "    unsup_dataloader = DataLoader(dataset=unsup_dataset, batch_size=int(config['batch_size']*config['unsup_data_ratio']), shuffle=True, num_workers=4, collate_fn=unsup_collate_fn)\n",
        "    \n",
        "  dev_dataloader = DataLoader(dataset=dev_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
        "  test_dataloader = DataLoader(dataset=test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
        "\n",
        "  return unsup_dataloader, train_dataloader, dev_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNFU7rpPsSf5",
        "outputId": "f323ee80-577c-489c-8da2-d78db71c9398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "unsup_dataloader, train_dataloader, dev_dataloader, test_dataloader = build_dataloader(config, data, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zD1RslZMsb-x",
        "outputId": "8510d4f8-eb99-4deb-f661-b11715eb3b29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[ 101,  955, 1446, 1372, 5543, 1146,  115,  115,  115, 3309, 6820,  749,\n",
            "          720,  102, 6010, 6009,  955, 1446, 6820, 3621, 5543, 1146, 1914, 2208,\n",
            "         3309,  102],\n",
            "        [ 101, 2769, 2682, 2828, 5381, 1555, 6587, 3121, 2768,  955, 1446,  102,\n",
            "          711,  784,  720, 6010, 6009,  955, 1446, 8024, 1359, 2768, 5381, 1555,\n",
            "         6587,  102],\n",
            "        [ 101, 2769, 4638, 5709, 1446, 6820, 3621, 5353, 1103,  749, 2582,  720,\n",
            "         1905, 4415,  102, 2582,  720, 2798, 5543, 6820, 5709, 1446,  677, 4638,\n",
            "         7178,  102],\n",
            "        [ 101,  711,  784,  720, 2769, 4638, 5709, 1446, 7360, 1168,  115,  115,\n",
            "          115,  102, 5709, 1446,  711,  784,  720, 7360,  856,  749,  671,  674,\n",
            "         1914,  102],\n",
            "        [ 101,  784,  720, 3198,  952, 2612, 1908, 6010, 6009, 5709, 1446, 7583,\n",
            "         2428,  886, 4500,  102, 2582,  720,  886, 5709, 1446, 2612, 1908,  886,\n",
            "         4500,  102],\n",
            "        [ 101, 2582,  720, 1357, 3867, 5709, 1446, 6158, 5632, 1220, 1041,  966,\n",
            "         6413, 6589,  102, 5709, 1446, 2582,  720, 1103,  679, 2768, 6413, 6589,\n",
            "          749,  102],\n",
            "        [ 101, 2769, 4638, 5709, 1446, 6820, 5543, 5314, 2769,  955, 4500,  749,\n",
            "         1408,  102, 2769, 4638, 5709, 1446, 6820, 5543, 1726, 1908,  886, 4500,\n",
            "         1408,  102],\n",
            "        [ 101, 5709, 1446, 1146, 3309,  743, 4638, 1555, 1501, 8024, 6842, 3621,\n",
            "         2582,  720, 1215,  102, 5709, 1446, 1146, 3309, 4638, 1963,  862, 6842,\n",
            "         3621,  102],\n",
            "        [ 101, 6010, 6009,  955, 1446, 1914,  719, 6397,  844,  671, 5011,  102,\n",
            "         5143, 5320,  671, 5663,  833, 1914,  719, 6397,  844,  671, 3613,  955,\n",
            "         1446,  102],\n",
            "        [ 101,  800, 6375, 2769, 1726, 6821,  702, 1384, 4638, 5709, 1446,  102,\n",
            "         2190, 3175, 4638, 1369,  671,  702, 6572, 1384, 2458, 6858,  749, 5709,\n",
            "         1446,  102],\n",
            "        [ 101, 5709, 1446, 3867, 6589, 1400, 2853, 1946, 4638,  691, 6205, 1762,\n",
            "         1525, 4692,  102, 5709, 1446, 3867, 6589,  749, 1762, 1525, 7027, 2853,\n",
            "         1946,  102],\n",
            "        [ 101, 2421, 7215,  738,  679, 5543, 2458, 6858, 5709, 1446, 3118,  802,\n",
            "         6821,  702, 1216, 5543,  102, 2458, 6858, 6010, 6009, 5709, 1446, 3118,\n",
            "          802,  102],\n",
            "        [ 101, 5709, 1446, 6206, 1059, 6956, 2990, 1184, 5310, 3926, 1762, 1525,\n",
            "         7027,  102, 5709, 1446, 2990, 1184, 5310, 3926, 1377,  809, 6848, 2885,\n",
            "         1408,  102],\n",
            "        [ 101, 2376, 3301, 1351, 6820,  749, 5709, 1446, 5543, 3059, 1726, 1408,\n",
            "          102, 2376, 6820, 5709, 1446, 6821,  702, 6913, 6435, 2582,  720, 3059,\n",
            "         1726,  102],\n",
            "        [ 101, 3800, 7218, 6572, 2787,  833, 2512, 1510, 5709, 1446, 1408,  102,\n",
            "         3800, 7218, 3118,  802, 2140, 6572, 2787,  833, 2512, 1510, 5709, 1446,\n",
            "         1408,  102],\n",
            "        [ 101,  865, 7583, 2140, 6760,  679, 6822, 1343, 2582,  720, 6820,  955,\n",
            "         1446,  102, 6760,  679, 1168,  865, 7583, 2140, 2582,  720, 6820,  955,\n",
            "         1446,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1]]), 'labels': tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1])}\n",
            "('input_ids', tensor([[ 101,  955, 1446, 1372, 5543, 1146,  115,  115,  115, 3309, 6820,  749,\n",
            "          720,  102, 6010, 6009,  955, 1446, 6820, 3621, 5543, 1146, 1914, 2208,\n",
            "         3309,  102],\n",
            "        [ 101, 2769, 2682, 2828, 5381, 1555, 6587, 3121, 2768,  955, 1446,  102,\n",
            "          711,  784,  720, 6010, 6009,  955, 1446, 8024, 1359, 2768, 5381, 1555,\n",
            "         6587,  102],\n",
            "        [ 101, 2769, 4638, 5709, 1446, 6820, 3621, 5353, 1103,  749, 2582,  720,\n",
            "         1905, 4415,  102, 2582,  720, 2798, 5543, 6820, 5709, 1446,  677, 4638,\n",
            "         7178,  102],\n",
            "        [ 101,  711,  784,  720, 2769, 4638, 5709, 1446, 7360, 1168,  115,  115,\n",
            "          115,  102, 5709, 1446,  711,  784,  720, 7360,  856,  749,  671,  674,\n",
            "         1914,  102],\n",
            "        [ 101,  784,  720, 3198,  952, 2612, 1908, 6010, 6009, 5709, 1446, 7583,\n",
            "         2428,  886, 4500,  102, 2582,  720,  886, 5709, 1446, 2612, 1908,  886,\n",
            "         4500,  102],\n",
            "        [ 101, 2582,  720, 1357, 3867, 5709, 1446, 6158, 5632, 1220, 1041,  966,\n",
            "         6413, 6589,  102, 5709, 1446, 2582,  720, 1103,  679, 2768, 6413, 6589,\n",
            "          749,  102],\n",
            "        [ 101, 2769, 4638, 5709, 1446, 6820, 5543, 5314, 2769,  955, 4500,  749,\n",
            "         1408,  102, 2769, 4638, 5709, 1446, 6820, 5543, 1726, 1908,  886, 4500,\n",
            "         1408,  102],\n",
            "        [ 101, 5709, 1446, 1146, 3309,  743, 4638, 1555, 1501, 8024, 6842, 3621,\n",
            "         2582,  720, 1215,  102, 5709, 1446, 1146, 3309, 4638, 1963,  862, 6842,\n",
            "         3621,  102],\n",
            "        [ 101, 6010, 6009,  955, 1446, 1914,  719, 6397,  844,  671, 5011,  102,\n",
            "         5143, 5320,  671, 5663,  833, 1914,  719, 6397,  844,  671, 3613,  955,\n",
            "         1446,  102],\n",
            "        [ 101,  800, 6375, 2769, 1726, 6821,  702, 1384, 4638, 5709, 1446,  102,\n",
            "         2190, 3175, 4638, 1369,  671,  702, 6572, 1384, 2458, 6858,  749, 5709,\n",
            "         1446,  102],\n",
            "        [ 101, 5709, 1446, 3867, 6589, 1400, 2853, 1946, 4638,  691, 6205, 1762,\n",
            "         1525, 4692,  102, 5709, 1446, 3867, 6589,  749, 1762, 1525, 7027, 2853,\n",
            "         1946,  102],\n",
            "        [ 101, 2421, 7215,  738,  679, 5543, 2458, 6858, 5709, 1446, 3118,  802,\n",
            "         6821,  702, 1216, 5543,  102, 2458, 6858, 6010, 6009, 5709, 1446, 3118,\n",
            "          802,  102],\n",
            "        [ 101, 5709, 1446, 6206, 1059, 6956, 2990, 1184, 5310, 3926, 1762, 1525,\n",
            "         7027,  102, 5709, 1446, 2990, 1184, 5310, 3926, 1377,  809, 6848, 2885,\n",
            "         1408,  102],\n",
            "        [ 101, 2376, 3301, 1351, 6820,  749, 5709, 1446, 5543, 3059, 1726, 1408,\n",
            "          102, 2376, 6820, 5709, 1446, 6821,  702, 6913, 6435, 2582,  720, 3059,\n",
            "         1726,  102],\n",
            "        [ 101, 3800, 7218, 6572, 2787,  833, 2512, 1510, 5709, 1446, 1408,  102,\n",
            "         3800, 7218, 3118,  802, 2140, 6572, 2787,  833, 2512, 1510, 5709, 1446,\n",
            "         1408,  102],\n",
            "        [ 101,  865, 7583, 2140, 6760,  679, 6822, 1343, 2582,  720, 6820,  955,\n",
            "         1446,  102, 6760,  679, 1168,  865, 7583, 2140, 2582,  720, 6820,  955,\n",
            "         1446,  102]]))\n",
            "('token_type_ids', tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1]]))\n",
            "('attention_mask', tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1]]))\n",
            "('labels', tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1]))\n"
          ]
        }
      ],
      "source": [
        "for i in train_dataloader:\n",
        "  print(i)\n",
        "  for j in i.items():\n",
        "    print(j)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrqI_bnbswfw"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score,accuracy_score\n",
        "def evaluation(config, model, val_dataloader):\n",
        "  model.eval()\n",
        "  preds = []\n",
        "  labels = []\n",
        "  val_loss = 0.\n",
        "  val_iterator = tqdm(val_dataloader, desc='Evaluation', total=len(val_dataloader))\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in val_iterator:  \n",
        "      labels.append(batch['labels'])\n",
        "      batch_cuda = {item: value.to(config['device']) for item,value in list(batch.items())}\n",
        "      batch_cuda['mode'] = 'val'\n",
        "    \n",
        "      loss, logits = model(**batch_cuda)[:2]\n",
        "\n",
        "      if config['n_gpus'] > 1:\n",
        "        loss = loss.mean()\n",
        "      val_loss += loss.item()\n",
        "      \n",
        "      preds.append(logits.argmax(dim=-1).detach().cpu())\n",
        "\n",
        "  avg_val_loss = val_loss/len(val_dataloader)\n",
        "  labels = torch.cat(labels, dim=0).numpy()\n",
        "  preds = torch.cat(preds, dim=0).numpy()\n",
        "\n",
        "  f1 = f1_score(labels, preds)\n",
        "  acc = accuracy_score(labels, preds)\n",
        "\n",
        "  return avg_val_loss, f1, acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfv4qecRxhv-"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "import torch.nn as nn\n",
        "class BertForAFQMC(BertForSequenceClassification):\n",
        "  def forward(self, input_ids, token_type_ids, attention_mask, labels=None, mode='train'):\n",
        "    outputs = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "\n",
        "    pooled_output = outputs[1]\n",
        "    pooled_output = self.dropout(pooled_output)\n",
        "\n",
        "    logits = self.classifier(pooled_output)\n",
        "    outputs = (logits, )\n",
        "\n",
        "    if mode == 'val':\n",
        "      loss_fct = nn.CrossEntropyLoss()\n",
        "      loss = loss_fct(logits, labels.view(-1))\n",
        "\n",
        "      outputs = (loss, ) + outputs\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6pipkLNxhyA"
      },
      "outputs": [],
      "source": [
        "def get_data(sup_batch, unsup_batch, config):\n",
        "  grad_data = {}\n",
        "  no_grad_data = {}\n",
        "\n",
        "  sup_max_len = sup_batch['input_ids'].size(1)\n",
        "  unsup_max_len = unsup_batch['ba_input_ids'].size(1)\n",
        "\n",
        "  cur_max_len = max(sup_max_len, unsup_max_len)\n",
        "\n",
        "  for item, sup_value in sup_batch.items():\n",
        "    if item == 'labels':\n",
        "      grad_data[item] = sup_value.to(config['device'])\n",
        "      continue\n",
        "    \n",
        "    ba_unsup_value = unsup_batch[f'ba_{item}']\n",
        "    ab_unsup_value = unsup_batch[f'ab_{item}']\n",
        "\n",
        "    if sup_max_len == cur_max_len:\n",
        "      padding_value = torch.zeros((ba_unsup_value.size(0), cur_max_len-unsup_max_len), dtype=ba_unsup_value.dtype)\n",
        "      ba_unsup_value = torch.cat([ba_unsup_value, padding_value], dim=-1)\n",
        "\n",
        "    else:\n",
        "      padding_value = torch.zeros((sup_value.size(0), cur_max_len-sup_max_len), dtype=sup_value.dtype)\n",
        "      sup_value = torch.cat([sup_value, padding_value], dim=-1)\n",
        "\n",
        "    grad_value = torch.cat([sup_value, ba_unsup_value], dim=0)\n",
        "\n",
        "    grad_data[item] = grad_value.to(config['device'])\n",
        "    no_grad_data[item] = ab_unsup_value.to(config['device'])\n",
        "\n",
        "  return grad_data, no_grad_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gztYvJwmxh2s"
      },
      "outputs": [],
      "source": [
        "def forward_no_grad(no_grad_data, config, model):\n",
        "  with torch.no_grad():\n",
        "    no_grad_logits = model(**no_grad_data)[0]\n",
        "\n",
        "    no_grad_probs = torch.softmax(no_grad_logits/config['uda_softmax_temp'], dim=1)\n",
        "\n",
        "    largest_probs, _ = no_grad_probs.max(dim=-1)\n",
        "    unsup_loss_mask = largest_probs.gt(config['uda_confidence_threshold']).float()\n",
        "\n",
        "  return unsup_loss_mask, no_grad_probs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spwWb-0X_gSu"
      },
      "outputs": [],
      "source": [
        "def get_tsa_threshold(total_steps, global_steps):\n",
        "  return np.exp((global_steps/ total_steps) * 5) / 2 + 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95GrFL8vxh5K"
      },
      "outputs": [],
      "source": [
        "def forward_with_grad(unsup_loss_mask, unsup_probs, config, cur_bs, model, grad_data, total_steps, global_steps):\n",
        "  tsa_threshold = get_tsa_threshold(total_steps, global_steps)\n",
        "\n",
        "  logits = model(**grad_data)[0]\n",
        "\n",
        "  sup_logits, unsup_logits = logits.split([logits.size(0)-cur_bs, cur_bs])\n",
        "\n",
        "  sup_labels = grad_data['labels'][:logits.size(0)-cur_bs]\n",
        "  per_example_loss = nn.CrossEntropyLoss(reduction='none')(sup_logits, sup_labels)\n",
        "\n",
        "  correct_label_probs = torch.softmax(sup_logits, dim=-1).gather(dim=-1, index=sup_labels.view(-1, 1))\n",
        "\n",
        "  sup_loss_mask = correct_label_probs.le(tsa_threshold).squeeze().float()\n",
        "\n",
        "  per_example_loss *= sup_loss_mask\n",
        "\n",
        "  sup_loss = per_example_loss.sum()/max(sup_loss_mask.sum(), 1)\n",
        "\n",
        "\n",
        "  unsup_log_probs = torch.log_softmax(unsup_logits, dim=1)\n",
        "\n",
        "  per_example_kl_loss = nn.KLDivLoss(reduction='none')(unsup_log_probs, unsup_probs).sum(dim=-1)\n",
        "\n",
        "  per_example_kl_loss *= unsup_loss_mask\n",
        "\n",
        "  unsup_loss = per_example_kl_loss.sum()/ max(unsup_loss_mask.sum(), 1)\n",
        "\n",
        "  loss = sup_loss + unsup_loss\n",
        "  \n",
        "  if config['n_gpus'] > 1:\n",
        "    loss = loss.mean()\n",
        "    sup_loss = sup_loss.mean()\n",
        "    unsup_loss = unsup_loss.mean()\n",
        "\n",
        "  return loss, tsa_threshold, unsup_loss, sup_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMLmDEfgCCzP"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW\n",
        "from tqdm import trange\n",
        "import os\n",
        "def train(config, train_dataloader, dev_dataloader, unsup_dataloader=None):\n",
        "  model = BertForAFQMC.from_pretrained(config['model_path'])\n",
        "\n",
        "  optimizer = AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
        "  model.to(config['device'])\n",
        "\n",
        "  total_steps = len(unsup_dataloader) * config['num_epoches']\n",
        "  epoch_iterator = trange(config['num_epoches'])\n",
        "  global_steps = 0\n",
        "  train_loss = 0.\n",
        "  logging_loss = 0.\n",
        "  best_acc = 0.\n",
        "  best_model_path = ''\n",
        "\n",
        "  if config['n_gpus'] > 1:\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "  train_iterator = iter(train_dataloader)\n",
        "\n",
        "  for _ in epoch_iterator:\n",
        "    unsup_iterator = tqdm(unsup_dataloader, desc='Training', total=len(unsup_dataloader))\n",
        "    model.train()\n",
        "\n",
        "    for unsup_batch in unsup_iterator:\n",
        "      cur_bs = unsup_batch['ab_input_ids'].size(0)\n",
        "      try:\n",
        "        sup_batch = next(train_iterator)\n",
        "      except StopIteration:\n",
        "        train_iterator = iter(train_dataloader)\n",
        "        sup_batch = next(train_iterator)\n",
        "\n",
        "      grad_data, no_grad_data = get_data(sup_batch, unsup_batch, config)\n",
        "\n",
        "      unsup_loss_mask, unsup_probs = forward_no_grad(no_grad_data, config, model)\n",
        "\n",
        "      loss, tsa_threshold, unsup_loss, sup_loss = forward_with_grad(\n",
        "          unsup_loss_mask, unsup_probs, config, cur_bs, model, grad_data, total_steps, global_steps\n",
        "      )\n",
        "\n",
        "      model.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "      optimizer.step()\n",
        "\n",
        "      if config['ema_start']:\n",
        "        ema.update()\n",
        "\n",
        "      train_loss += loss.item()\n",
        "      global_steps +=1\n",
        "\n",
        "      unsup_iterator.set_postfix_str(f'running training loss: {loss.item():.4f}')\n",
        "\n",
        "      if global_steps % config['logging_step'] == 0:\n",
        "        if global_steps >= config['ema_start_step'] and not config['ema_start']:\n",
        "          print('\\n>>> EMA starting ...')\n",
        "          config['ema_start'] = True\n",
        "          ema = EMA(model.module if hasattr(model, 'module') else model, decay=0.999)\n",
        "\n",
        "        print_train_loss = (train_loss - logging_loss) / config['logging_step']\n",
        "        logging_loss = train_loss\n",
        "\n",
        "        if config['ema_start']:\n",
        "          ema.apply_shadow()\n",
        "        val_loss, f1, acc = evaluation(config, model, dev_dataloader)\n",
        "\n",
        "        print_log = f'\\n>>> training loss: {print_train_loss:.6f}, valid loss: {val_loss:.6f}'\n",
        "\n",
        "        if acc > best_acc:\n",
        "          model_save_path = os.path.join(config['output_path'], f'checkpoint-{global_steps}-{acc:.6f}')\n",
        "          model_to_save = model.module if hasattr(model, 'module') else model_config\n",
        "          model_to_save.save_pretrained(model_save_path)\n",
        "          best_acc = acc\n",
        "          best_model_path = model_save_path\n",
        "\n",
        "        print_log += f'valid f1: {f1:.6f}, valid acc: {acc:.6f}'\n",
        "        print(print_log)\n",
        "        model.train()\n",
        "        if config['ema_start']:\n",
        "          ema.restore()\n",
        "\n",
        "  return model, best_model_path\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fW8CsUw5Hc4E"
      },
      "outputs": [],
      "source": [
        "model, best_model_path = train(config, train_dataloader, dev_dataloader, unsup_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfzYzyqbHmRW"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}