{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "头条新闻分类_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3JTQk9T_L1i"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "config = {\n",
        "    'train_file_path': '/content/drive/MyDrive/data/train.csv',\n",
        "    'test_file_path': '/content/drive/MyDrive/data/test.csv',\n",
        "    'train_val_ratio': 0.1,\n",
        "    # 'vocab_size': 30000,\n",
        "    'model_path': '/content/drive/MyDrive/BERT_model',\n",
        "    'batch_size': 16,\n",
        "    'num_epoches': 2,\n",
        "    'learning_rate': 1e-3,\n",
        "    'logging_step': 300,\n",
        "    'seed': 2021\n",
        "}\n",
        "\n",
        "config['device'] = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def seed_everything(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(config['seed'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "def read_data(config, tokenizer, mode='train'):\n",
        "  data_df = pd.read_csv(config[f'{mode}_file_path'], sep=',')\n",
        "\n",
        "  if mode == 'train':\n",
        "    X_train, y_train = defaultdict(list), []\n",
        "    X_val, y_val = defaultdict(list), []\n",
        "\n",
        "    num_val = int(config['train_val_ratio'] * len(data_df))\n",
        "  \n",
        "  else:\n",
        "    X_test, y_test = defaultdict(list), []\n",
        "\n",
        "  for i, row in tqdm(data_df.iterrows(), desc=f'Preprocesing {mode} data', total=len(data_df)):\n",
        "    label = row[1] if mode=='train' else 0\n",
        "    sentence = row[-1]\n",
        "#-----------------------#\n",
        "    inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, return_token_type_ids=True, return_attention_mask=True)\n",
        "#-----------------------#\n",
        "    if mode == 'train':\n",
        "      if i < num_val:\n",
        "        X_val['input_ids'].append(inputs['input_ids'])\n",
        "        y_val.append(label)\n",
        "        X_val['token_type_ids'].append(inputs['token_type_ids'])\n",
        "        X_val['attention_mask'].append(inputs['attention_mask'])\n",
        "      else:\n",
        "        X_train['input_ids'].append(inputs['input_ids'])\n",
        "        y_train.append(label)\n",
        "        X_train['token_type_ids'].append(inputs['token_type_ids'])\n",
        "        X_train['attention_mask'].append(inputs['attention_mask'])\n",
        "\n",
        "    else:\n",
        "      X_test['input_ids'].append(inputs['input_ids'])\n",
        "      y_test.append(label)\n",
        "      X_test['token_type_ids'].append(inputs['token_type_ids'])\n",
        "      X_test['attention_mask'].append(inputs['attention_mask'])\n",
        "\n",
        "\n",
        "  if mode == 'train':\n",
        "    label2id = {label: i for i, label in enumerate(np.unique(y_train))}\n",
        "    id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "    y_train = torch.tensor([label2id[label] for label in y_train], dtype=torch.long)\n",
        "    y_val = torch.tensor([label2id[label] for label in y_val], dtype=torch.long)\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, label2id, id2label\n",
        "  \n",
        "  else:\n",
        "    y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "    return X_test, y_test"
      ],
      "metadata": {
        "id": "GotnDIQZ_L1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "class TNEWSDataset(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.x = X\n",
        "    self.y = y\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return{\n",
        "        'input_ids': self.x['input_ids'][idx],\n",
        "        'label': self.y[idx],\n",
        "        'token_type_ids': self.x['token_type_ids'][idx],\n",
        "        'attention_mask': self.x['attention_mask'][idx]\n",
        "    }\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.y.size(0)\n"
      ],
      "metadata": {
        "id": "RzAfIpa8_L1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(examples):\n",
        "  input_ids_list = []\n",
        "  labels = []\n",
        "  token_type_ids_list = []\n",
        "  attention_mask_list = []\n",
        "\n",
        "  for example in examples:\n",
        "    input_ids_list.append(example['input_ids'])\n",
        "    labels.append(example['label'])\n",
        "    token_type_ids_list.append(example['token_type_ids'])\n",
        "    attention_mask_list.append(example['attention_mask'])\n",
        "\n",
        "  max_length = max(len(input_ids) for input_ids in input_ids_list)\n",
        "\n",
        "  input_ids_tensor = torch.zeros((len(labels), max_length), dtype=torch.long)\n",
        "  token_type_ids_tensor = torch.zeros_like(input_ids_tensor)\n",
        "  attention_mask_tensor = torch.zeros_like(input_ids_tensor)\n",
        "\n",
        "  for i, input_ids in enumerate(input_ids_list):\n",
        "    seq_len = len(input_ids)\n",
        "    input_ids_tensor[i, : seq_len] = torch.tensor(input_ids, dtype=torch.long)\n",
        "    token_type_ids_tensor[i, : seq_len] = torch.tensor(token_type_ids_list[i], dtype=torch.long)\n",
        "    attention_mask_tensor[i, : seq_len] = torch.tensor(attention_mask_list[i], dtype=torch.long)\n",
        "\n",
        "  return{\n",
        "      'input_ids': input_ids_tensor,\n",
        "      'labels': torch.tensor(labels, dtype=torch.long),\n",
        "      'token_type_ids': token_type_ids_tensor,\n",
        "      'attention_mask': attention_mask_tensor\n",
        "  }\n"
      ],
      "metadata": {
        "id": "tOpz1mqS_L1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizer\n",
        "def build_dataloader(config):\n",
        "#-----------------------#\n",
        "  tokenizer = BertTokenizer.from_pretrained(config['model_path'])\n",
        "#-----------------------#\n",
        "  X_train, y_train, X_val, y_val, label2id, id2label = read_data(config, tokenizer, mode='train')\n",
        "  X_test, y_test = read_data(config, tokenizer, mode='test')\n",
        "\n",
        "  train_dataset = TNEWSDataset(X_train, y_train)\n",
        "  val_dataset = TNEWSDataset(X_val, y_val)\n",
        "  test_dataset = TNEWSDataset(X_test, y_test)\n",
        "\n",
        "  train_dataloader = DataLoader(dataset=train_dataset, batch_size=config['batch_size'], num_workers=4, shuffle=True, collate_fn=collate_fn)\n",
        "  val_dataloader = DataLoader(dataset=val_dataset, batch_size=config['batch_size'], num_workers=4, shuffle=False, collate_fn=collate_fn)\n",
        "  test_dataloader = DataLoader(dataset=test_dataset, batch_size=config['batch_size'], num_workers=4, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "  return id2label, train_dataloader, val_dataloader, test_dataloader"
      ],
      "metadata": {
        "id": "NO0QxAzW_L1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label, train_dataloader, val_dataloader, test_dataloader = build_dataloader(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5d2c0ea-8b6c-4fc5-9935-10c436cfd04f",
        "id": "WvfdTsGV_L1o"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preprocesing train data: 100%|██████████| 63360/63360 [00:49<00:00, 1280.30it/s]\n",
            "Preprocesing test data: 100%|██████████| 10000/10000 [00:05<00:00, 1757.35it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in test_dataloader:\n",
        "  print(len(batch['input_ids']))\n",
        "  print(batch)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2df7d14a-60bb-4f84-e7fb-1a8cd41a932a",
        "id": "HSJD30C6_L1o"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "{'input_ids': tensor([[  101,   143,  5500,  8038,   123,   702,  5301,  1146,  7566,  1818,\n",
            "          7987,  1928,   702,  5500,   966,  2533,  5500,  3696,  1068,  3800,\n",
            "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [  101,   743,  1947,  2791,   679,  7676,  1408,  8043,   711,   784,\n",
            "           720,   833,  3300,   782,  2703,  5709,  8298,   674,   743,  6956,\n",
            "          2797,  3322,  8043,   102,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [  101,  8212,  1914,  2157,  2791,  1765,   772,  1062,  1385,  6760,\n",
            "          6121,  1075,  4343,  8043,   872,  4692,  1168,  4638,   788,   788,\n",
            "          3221,  6134,  7481,   102,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [  101,   523,  2900,  3144,  4764,  5296,  4788,   855,  1400,  4638,\n",
            "          2590,  6662,  3463,  4415,   524,   102,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [  101,  1762,   100,  2714,   100,  4638,  6814,  4923,   704,  4916,\n",
            "          3104,   100,  2571,   100,  4638,  4255,  1355,  1213,   102,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [  101,  1139,  2345,  8211,  3613,   510,  7607,  6121, 12458,  9099,\n",
            "          1062,  7027,  8024,  2769,  4692,  1168,   749,   704,  1744,  3297,\n",
            "          4696,  2141,  4638,  6577,  1737,   102,     0,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [  101,   676,  2108,  2428,  8421,  1872,  7270,   125,   119,   130,\n",
            "           110,  8024,  1184,   676,  2108,  2428,  5307,  3845,  1872,  7270,\n",
            "          4507,  6566,  6760,  3633,   102,     0,     0,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [  101,  6924,  1290,  8038,  3121,  1359,  3688,  3813,   691,  1092,\n",
            "           808,  2802,  1921,  3823,  8024,  6399,  4788,  5401,  1092,   785,\n",
            "          2335,  4633,  7355,  8024,  2510,  2548,  2577,  6614,  1962,  2376,\n",
            "          2797,   102],\n",
            "        [  101,  1384,  4917,   791,  2399,  3297,  1400,   671,  4275,  7213,\n",
            "          3331,  1383,  8043,  1920,  7434,  5688,  3698,  4638,  7770,  2275,\n",
            "          2162,  7415,  8024,  1367,  3333,  2226,  2372,  7942,  7032,  4508,\n",
            "           102,     0],\n",
            "        [  101,  4060,  3730,   782,  5307,  1555,  3300,   784,   720,  4908,\n",
            "          6394,  8043,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [  101,   794,  5661,  3678,   677,  6663,  3717,  1314,  7372,  1408,\n",
            "          8043,  6230,  2533,  1469,   794,  3807,  3737,  6663,  3352,   677,\n",
            "          6663,  3717,   671,  3416,  8024,  2218,  1920,  7231,  4294,  7231,\n",
            "           749,   102],\n",
            "        [  101,  6435,  2828,  2797,  3123,  1168,  3430,  2094,  2419,   678,\n",
            "          8020,  8110,  3299,  8108,  3189,  8021,   102,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [  101,  2630,   782,  4638,  7434,  8020,  1333,  1158,  6408,  3625,\n",
            "          8021,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [  101,  4696,  7676,  8013,  7434,  1818,  2109,  2270,  6804,  7344,\n",
            "          6825,  1391,   677,  4125,  7222,  8024,  6134,  2658,   778,   749,\n",
            "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [  101,  2476,   671,  2255,   517,  7922,  7959,  6381,   518,  2130,\n",
            "          2768,  3131,  6604,  8024,  7504,  2207,  2140,  2307,  6629,   749,\n",
            "          8024,  2456,  2123,  2141,  1213,  4028,  5306,   102,     0,     0,\n",
            "             0,     0],\n",
            "        [  101,   517,  3198,   807,  1453,  1149,   518,  5314,   100,  8439,\n",
            "           100,  4514,   749,   100,  5273,   166,   100,  8024,  1380,   677,\n",
            "           788,  1139,  4385,  6814,  1724,  3613,   102,     0,     0,     0,\n",
            "             0,     0]]), 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 0, 0, 0, 0, 0]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "def evaluation(model, config, val_dataloader):\n",
        "  model.eval()\n",
        "  preds = []\n",
        "  labels = []\n",
        "  val_loss = 0.\n",
        "  val_iterator = tqdm(val_dataloader, desc='Evaluation', total=len(val_dataloader))\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in val_iterator:\n",
        "      labels.append(batch['labels'])\n",
        "      batch = {item: value.to(config['device']) for item, value in batch.items()}\n",
        "\n",
        "      loss, logits = model(**batch)[:2]\n",
        "      val_loss += loss.item()\n",
        "      \n",
        "      preds.append(logits.argmax(dim=-1).detach().cpu())\n",
        "\n",
        "  avg_val_loss = val_loss/len(val_dataloader)\n",
        "  labels = torch.cat(labels, dim=0).numpy()\n",
        "  preds = torch.cat(preds, dim=0).numpy()\n",
        "\n",
        "  f1 = f1_score(labels, preds, average='macro')\n",
        "\n",
        "  return avg_val_loss, f1\n"
      ],
      "metadata": {
        "id": "JKVSb1rE_L1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertConfig, BertForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from tqdm import trange\n",
        "def train(id2label, config, train_dataloader, val_dataloader):\n",
        "#-----------------------#\n",
        "  bert_config = BertConfig.from_pretrained(config['model_path'])\n",
        "  bert_config.num_labels = len(id2label)\n",
        "  model = BertForSequenceClassification.from_pretrained(config['model_path'], config=bert_config)\n",
        "#-----------------------#  \n",
        "\n",
        "  optimizer = AdamW(model.parameters(), lr=config['learning_rate'])\n",
        "  model.to(config['device'])\n",
        "  epoches_iterator = trange(config['num_epoches'])\n",
        "\n",
        "  global_steps = 0\n",
        "  train_loss = 0.\n",
        "  logging_loss = 0.\n",
        "\n",
        "\n",
        "  for epoch in epoches_iterator:\n",
        "    train_iterator = tqdm(train_dataloader, desc='Training', total=len(train_dataloader))\n",
        "    model.train()\n",
        "    \n",
        "    for batch in train_iterator:\n",
        "      batch = {item: value.to(config['device']) for item, value in batch.items()}\n",
        "\n",
        "      loss = model(**batch)[0]\n",
        "\n",
        "\n",
        "      model.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      train_loss += loss\n",
        "      global_steps +=1\n",
        "\n",
        "      if global_steps % config['logging_step'] == 0:\n",
        "        print_train_loss = (train_loss - logging_loss)/ config['logging_step'] \n",
        "        logging_loss = train_loss\n",
        "\n",
        "        avg_val_loss, f1 = evaluation(model, config, val_dataloader)\n",
        "        print(avg_val_loss, f1)\n",
        "        model.train()\n",
        "        \n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "uWZ6iykI_L1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = train(id2label, config, train_dataloader, val_dataloader)"
      ],
      "metadata": {
        "id": "TEPhbilm_L1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(config, id2label, model, test_dataloader):\n",
        "  model.eval()\n",
        "  test_iterator = tqdm(test_dataloader, desc='Predicting', total=len(test_dataloader))\n",
        "  test_preds =[]\n",
        "\n",
        "  with torch.no_grad():\n",
        "     for batch in test_iterator:\n",
        "       batch = {item:value.to(config['device']) for item, value in batch.items()}\n",
        "       logits = model(**batch)[1]\n",
        "       test_preds.append(logits.argmax(dim=-1).detach().cpu())\n",
        "  test_preds = torch.cat(test_preds, dim=0).numpy()\n",
        "  test_preds = [id2label[idx] for idx in test_preds]\n",
        "\n",
        "  test_df = pd.read_csv(config['test_file_path'], sep=',')\n",
        "  test_df['preds'] = test_preds\n",
        "  test_df.to_csv('/content/drive/MyDrive/BERT_result.csv', index=False, encoding='utf8')\n",
        "  return test_df"
      ],
      "metadata": {
        "id": "Yim07O-D_L1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = predict(config, id2label, best_model, test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c87a0b88-8835-4d52-9a90-d9148b57b277",
        "id": "Jp2l6gpN_L1p"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rPredicting:   0%|          | 0/625 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Predicting: 100%|██████████| 625/625 [00:50<00:00, 12.40it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vCvP5SKuKA3n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}